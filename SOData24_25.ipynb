{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPioAZ/Rbmb3Ynj0246ald9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apenas1Silva/SOData_2025/blob/main/SOData24_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# State of Data Brazil 2024-2025\n",
        "\n",
        "*1 - Sobre a Pesquisa*\n",
        "\n",
        "Entre novembro e dezembro de 2024, mais de 5,2 mil profissionais responderam o State of Data Brazil -a pesquisa anual, do maior panorama do mercado de trabalho em dados no Brasil -, feito pela comunidade Data Hackers em parceria com a Bain & Company, esta é a 5ª edição desta pesquisa que teve início em 2019.\n",
        "\n",
        "\n",
        "Veja mais detalhes no site da pesquisa: stateofdata.datahackers.com.br - No site da pesquisa você também pode fazer o download gratuito de um relatório completo desenvolvido pela Bain & Company com conclusões interessantes sobre o mercado de trabalho brasileiro na área de dados.\n",
        "\n",
        "\n",
        "A presente pesquisa é o resultado de um esforço conjunto da Data Hackers, a maior comunidade de dados do Brasil, e da Bain & Company, consultoria global que ajuda empresas e organizações a promover mudanças que definam o futuro dos negócios, para mapear o mercado de trabalho de dados no Brasil.\n",
        "\n",
        "A pesquisa foi conduzida entre 14 de outubro de 2024 e 18 de dezembro de 2024 e contou com 5.217\n",
        "respondentes brasileiros. A amostra reflete a visão de variados papéis de atuação em empresas, como os de\n",
        "analista de dados, cientista de dados e engenheiro de dados, bem como diferentes perfis de experiência\n",
        "profissional, incluindo analistas júnior, pleno, sênior e gestores.\n",
        "\n",
        "Gostaríamos de agradecer a toda a comunidade Data Hackers e a todos os parceiros que nos apoiaram durante a pesquisa, incluindo André Sionek, Karine Lago, Canal Let's Data, Canal Programação Dinâmica, Canal do Mario Filho, Alura, Canal Estatidados, Comunidade Mulheres em Dados, Teo Calvo, Flai, QuantitativaMente, Preditiva e professora Fernanda Maciel, sem o apoio de vocês nunca teríamos chegado a uma pesquisa tão completa e abrangente.\n",
        "\n",
        "2 Sobre o processamento e anonimização dos dados\n",
        "O dataset foi anonimizado com o objetivo de garantir a privacidade dos respondentes, para isso foi necessário em alguns casos remover outliers que poderiam identificar o entrevistado e, portanto, nem todos os dados coletados na pesquisa estarão disponíveis aqui. Estados com menor incidência de resposta, como alguns da região Norte por exemplo, terão apenas sua região indicada no dataset, também como consequência do processo de anonimização, o mesmo aconteceu em algumas outras perguntas.\n",
        "\n",
        "As perguntas cujas respostas são multi-valoradas ocupam mais de uma coluna no dataset. Portanto, para diferenciar quais colunas pertencem a quais perguntas, cada coluna é identificada com uma tupla. Sendo o primeiro identificador o da pergunta, e, no caso de várias respostas, o segundo identificador referencia a alternativa escolhida. As perguntas mapeadas são mostradas abaixo (lembrando que algumas foram removidas e outras tiveram alguns outliers transformados/apagados no processo de anonimização)\n",
        "\n",
        "3 Sobre os dados da Pesquisa\n",
        "O questionário foi dividido em 8 partes, e dentro de cada uma das partes temos as perguntas e opções de escolha.\n",
        "\n",
        "Parte 1 - Dados demográficos\n",
        "\n",
        "Parte 2 - Dados sobre carreira\n",
        "\n",
        "Parte 3 - Desafios dos gestores de times de dados\n",
        "\n",
        "Parte 4 - Conhecimentos na área de dados\n",
        "\n",
        "Parte 5 - Objetivos na área de dados\n",
        "\n",
        "Parte 6 - Conhecimentos em Engenharia de Dados/DE\n",
        "\n",
        "Parte 7 - Conhecimentos em Análise de Dados/DA\n",
        "\n",
        "Parte 8 - Conhecimentos em Ciências de Dados/DS\n",
        "\n",
        "Cada pergunta é dividida em Parte, Letra da Pergunta, Número da Opção escolhida\n",
        "Exemplo: P3a_1 = Parte 3, pergunta (a), opção (1)"
      ],
      "metadata": {
        "id": "Uvu4KWV6hZ8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando as Bibliotecas\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "import geopandas as gpd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "color_pal = sns.color_palette()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message=\"use_inf_as_na option is deprecated\")"
      ],
      "metadata": {
        "id": "9L1yDSiLhTqj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMtpnUsbf-2M",
        "outputId": "5766fe1c-bc68-4f4e-cc4e-eb662037fbd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/datahackers/state-of-data-brazil-20242025?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.64M/1.64M [00:00<00:00, 96.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Arquivos disponíveis: ['Final Dataset - State of Data 2024 - Kaggle - df_survey_2024.csv']\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/datahackers/state-of-data-brazil-20242025/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Definindo o endereço do Data Set\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"datahackers/state-of-data-brazil-20242025\")\n",
        "\n",
        "# Listar os arquivos dentro desse diretório\n",
        "files = os.listdir(path)\n",
        "print(\"Arquivos disponíveis:\", files)\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path+'/Final Dataset - State of Data 2024 - Kaggle - df_survey_2024.csv', sep=',')"
      ],
      "metadata": {
        "id": "UJOEUhO9qmDz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73vDLx3thquO",
        "outputId": "17cce841-45a4-43aa-c420-2bee8dea1b4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5217, 403)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqzQIDLThyTp",
        "outputId": "e4b537c9-10bc-4bb2-b970-c1ae1024368c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['0.a_token', '0.d_data/hora_envio', '1.a_idade', '1.a.1_faixa_idade',\n",
              "       '1.b_genero', '1.c_cor/raca/etnia', '1.d_pcd',\n",
              "       '1.e_experiencia_profissional_prejudicada',\n",
              "       '1.e.1_Não acredito que minha experiência profissional seja afetada',\n",
              "       '1.e.2_Sim, devido a minha Cor/Raça/Etnia',\n",
              "       ...\n",
              "       '8.d.3_Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.',\n",
              "       '8.d.4_Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).',\n",
              "       '8.d.5_Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.',\n",
              "       '8.d.6_Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.',\n",
              "       '8.d.7_Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.',\n",
              "       '8.d.8_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.',\n",
              "       '8.d.9_Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.',\n",
              "       '8.d.10_Criando e gerenciando soluções de Feature Store e cultura de MLOps.',\n",
              "       '8.d.11_Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)',\n",
              "       '8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.'],\n",
              "      dtype='object', length=403)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "YdGcauNAinEV",
        "outputId": "af7b290b-9b37-4e02-d97c-d71201a2536f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5217 entries, 0 to 5216\n",
            "Columns: 403 entries, 0.a_token to 8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.\n",
            "dtypes: bool(1), float64(323), int64(1), object(78)\n",
            "memory usage: 16.0+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.a_token                                                                                                             object\n",
              "0.d_data/hora_envio                                                                                                   object\n",
              "1.a_idade                                                                                                              int64\n",
              "1.a.1_faixa_idade                                                                                                     object\n",
              "1.b_genero                                                                                                            object\n",
              "                                                                                                                      ...   \n",
              "8.d.8_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.             float64\n",
              "8.d.9_Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.                                   float64\n",
              "8.d.10_Criando e gerenciando soluções de Feature Store e cultura de MLOps.                                           float64\n",
              "8.d.11_Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)    float64\n",
              "8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.                                             float64\n",
              "Length: 403, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.a_token</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.d_data/hora_envio</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.a_idade</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.a.1_faixa_idade</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.b_genero</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.d.8_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.d.9_Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.d.10_Criando e gerenciando soluções de Feature Store e cultura de MLOps.</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.d.11_Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>403 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "ePumPHzXwuos",
        "outputId": "9d8abda0-6993-4861-a0f1-f30fb9cde059"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          0.a_token  0.d_data/hora_envio  1.a_idade  \\\n",
              "0  reb94rv0msth7q4nreb94riaq80iz3yi  16/10/2024 11:19:17         18   \n",
              "1  1zc66g69jjt49y32l1zc66g8wqj79m4e  16/10/2024 20:45:31         18   \n",
              "2  uu99wmam4n5kc2uu99wmydf0rk7l58f7  17/10/2024 18:10:59         18   \n",
              "3  3ynsw7z0hl5hhpbfvaeqk73ynsw7z04l  22/10/2024 18:03:00         18   \n",
              "4  v6bji2ct5xckpl1uouv6bjiotkrf3b4f  23/10/2024 01:33:23         18   \n",
              "5  jct4toasj61rygexnyug3zjct4toascu  25/10/2024 12:02:06         18   \n",
              "6  vcvc4otfqznwludyp3mvcvc8euvs299t  12/11/2024 19:45:44         18   \n",
              "7  7o0jgas7pedzh1dr17o0jgzt0irkbqo6  21/11/2024 05:13:09         18   \n",
              "8  gmkbjmsg8fzfqljgmkbjmkr5zfkex28o  09/12/2024 12:16:43         18   \n",
              "9  vwsjeyi7044yittt3zdwvwsjefc53s6y  09/12/2024 14:03:12         18   \n",
              "\n",
              "  1.a.1_faixa_idade 1.b_genero 1.c_cor/raca/etnia 1.d_pcd  \\\n",
              "0             17-21  Masculino             Branca     Não   \n",
              "1             17-21  Masculino             Branca     Não   \n",
              "2             17-21  Masculino              Parda     Não   \n",
              "3             17-21  Masculino             Branca     Não   \n",
              "4             17-21  Masculino             Branca     Não   \n",
              "5             17-21  Masculino              Parda     Não   \n",
              "6             17-21  Masculino              Parda     Sim   \n",
              "7             17-21  Masculino              Parda     Não   \n",
              "8             17-21  Masculino              Parda     Não   \n",
              "9             17-21  Masculino              Parda     Não   \n",
              "\n",
              "            1.e_experiencia_profissional_prejudicada  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2  Não acredito que minha experiência profissiona...   \n",
              "3                                                NaN   \n",
              "4                                                NaN   \n",
              "5  Não acredito que minha experiência profissiona...   \n",
              "6  Não acredito que minha experiência profissiona...   \n",
              "7  Não acredito que minha experiência profissiona...   \n",
              "8  Não acredito que minha experiência profissiona...   \n",
              "9  Não acredito que minha experiência profissiona...   \n",
              "\n",
              "   1.e.1_Não acredito que minha experiência profissional seja afetada  \\\n",
              "0                                                NaN                    \n",
              "1                                                NaN                    \n",
              "2                                               1.00                    \n",
              "3                                                NaN                    \n",
              "4                                                NaN                    \n",
              "5                                               1.00                    \n",
              "6                                               1.00                    \n",
              "7                                               1.00                    \n",
              "8                                               1.00                    \n",
              "9                                               1.00                    \n",
              "\n",
              "   1.e.2_Sim, devido a minha Cor/Raça/Etnia  ...  \\\n",
              "0                                       NaN  ...   \n",
              "1                                       NaN  ...   \n",
              "2                                      0.00  ...   \n",
              "3                                       NaN  ...   \n",
              "4                                       NaN  ...   \n",
              "5                                      0.00  ...   \n",
              "6                                      0.00  ...   \n",
              "7                                      0.00  ...   \n",
              "8                                      0.00  ...   \n",
              "9                                      0.00  ...   \n",
              "\n",
              "   8.d.3_Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.  \\\n",
              "0                                                NaN                                                                                   \n",
              "1                                                NaN                                                                                   \n",
              "2                                                NaN                                                                                   \n",
              "3                                                NaN                                                                                   \n",
              "4                                                NaN                                                                                   \n",
              "5                                                NaN                                                                                   \n",
              "6                                                NaN                                                                                   \n",
              "7                                                NaN                                                                                   \n",
              "8                                                NaN                                                                                   \n",
              "9                                                NaN                                                                                   \n",
              "\n",
              "   8.d.4_Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).  \\\n",
              "0                                                NaN                                                                        \n",
              "1                                                NaN                                                                        \n",
              "2                                                NaN                                                                        \n",
              "3                                                NaN                                                                        \n",
              "4                                                NaN                                                                        \n",
              "5                                                NaN                                                                        \n",
              "6                                                NaN                                                                        \n",
              "7                                                NaN                                                                        \n",
              "8                                                NaN                                                                        \n",
              "9                                                NaN                                                                        \n",
              "\n",
              "  8.d.5_Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.  \\\n",
              "0                                                NaN                                                     \n",
              "1                                                NaN                                                     \n",
              "2                                                NaN                                                     \n",
              "3                                                NaN                                                     \n",
              "4                                                NaN                                                     \n",
              "5                                                NaN                                                     \n",
              "6                                                NaN                                                     \n",
              "7                                                NaN                                                     \n",
              "8                                                NaN                                                     \n",
              "9                                                NaN                                                     \n",
              "\n",
              "   8.d.6_Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.  \\\n",
              "0                                                NaN                                                                                                \n",
              "1                                                NaN                                                                                                \n",
              "2                                                NaN                                                                                                \n",
              "3                                                NaN                                                                                                \n",
              "4                                                NaN                                                                                                \n",
              "5                                                NaN                                                                                                \n",
              "6                                                NaN                                                                                                \n",
              "7                                                NaN                                                                                                \n",
              "8                                                NaN                                                                                                \n",
              "9                                                NaN                                                                                                \n",
              "\n",
              "   8.d.7_Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.  \\\n",
              "0                                                NaN                                                           \n",
              "1                                                NaN                                                           \n",
              "2                                                NaN                                                           \n",
              "3                                                NaN                                                           \n",
              "4                                                NaN                                                           \n",
              "5                                                NaN                                                           \n",
              "6                                                NaN                                                           \n",
              "7                                                NaN                                                           \n",
              "8                                                NaN                                                           \n",
              "9                                                NaN                                                           \n",
              "\n",
              "   8.d.8_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.  \\\n",
              "0                                                NaN                                                          \n",
              "1                                                NaN                                                          \n",
              "2                                                NaN                                                          \n",
              "3                                                NaN                                                          \n",
              "4                                                NaN                                                          \n",
              "5                                                NaN                                                          \n",
              "6                                                NaN                                                          \n",
              "7                                                NaN                                                          \n",
              "8                                                NaN                                                          \n",
              "9                                                NaN                                                          \n",
              "\n",
              "   8.d.9_Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.  \\\n",
              "0                                                NaN                                    \n",
              "1                                                NaN                                    \n",
              "2                                                NaN                                    \n",
              "3                                                NaN                                    \n",
              "4                                                NaN                                    \n",
              "5                                                NaN                                    \n",
              "6                                                NaN                                    \n",
              "7                                                NaN                                    \n",
              "8                                                NaN                                    \n",
              "9                                                NaN                                    \n",
              "\n",
              "   8.d.10_Criando e gerenciando soluções de Feature Store e cultura de MLOps.  \\\n",
              "0                                                NaN                            \n",
              "1                                                NaN                            \n",
              "2                                                NaN                            \n",
              "3                                                NaN                            \n",
              "4                                                NaN                            \n",
              "5                                                NaN                            \n",
              "6                                                NaN                            \n",
              "7                                                NaN                            \n",
              "8                                                NaN                            \n",
              "9                                                NaN                            \n",
              "\n",
              "   8.d.11_Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)  \\\n",
              "0                                                NaN                                                                   \n",
              "1                                                NaN                                                                   \n",
              "2                                                NaN                                                                   \n",
              "3                                                NaN                                                                   \n",
              "4                                                NaN                                                                   \n",
              "5                                                NaN                                                                   \n",
              "6                                                NaN                                                                   \n",
              "7                                                NaN                                                                   \n",
              "8                                                NaN                                                                   \n",
              "9                                                NaN                                                                   \n",
              "\n",
              "   8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.  \n",
              "0                                                NaN                         \n",
              "1                                                NaN                         \n",
              "2                                                NaN                         \n",
              "3                                                NaN                         \n",
              "4                                                NaN                         \n",
              "5                                                NaN                         \n",
              "6                                                NaN                         \n",
              "7                                                NaN                         \n",
              "8                                                NaN                         \n",
              "9                                                NaN                         \n",
              "\n",
              "[10 rows x 403 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a370d57a-ec1d-4435-bfea-2343cee4ea55\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.a_token</th>\n",
              "      <th>0.d_data/hora_envio</th>\n",
              "      <th>1.a_idade</th>\n",
              "      <th>1.a.1_faixa_idade</th>\n",
              "      <th>1.b_genero</th>\n",
              "      <th>1.c_cor/raca/etnia</th>\n",
              "      <th>1.d_pcd</th>\n",
              "      <th>1.e_experiencia_profissional_prejudicada</th>\n",
              "      <th>1.e.1_Não acredito que minha experiência profissional seja afetada</th>\n",
              "      <th>1.e.2_Sim, devido a minha Cor/Raça/Etnia</th>\n",
              "      <th>...</th>\n",
              "      <th>8.d.3_Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.</th>\n",
              "      <th>8.d.4_Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).</th>\n",
              "      <th>8.d.5_Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.</th>\n",
              "      <th>8.d.6_Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.</th>\n",
              "      <th>8.d.7_Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.</th>\n",
              "      <th>8.d.8_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.</th>\n",
              "      <th>8.d.9_Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.</th>\n",
              "      <th>8.d.10_Criando e gerenciando soluções de Feature Store e cultura de MLOps.</th>\n",
              "      <th>8.d.11_Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)</th>\n",
              "      <th>8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>reb94rv0msth7q4nreb94riaq80iz3yi</td>\n",
              "      <td>16/10/2024 11:19:17</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Branca</td>\n",
              "      <td>Não</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1zc66g69jjt49y32l1zc66g8wqj79m4e</td>\n",
              "      <td>16/10/2024 20:45:31</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Branca</td>\n",
              "      <td>Não</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>uu99wmam4n5kc2uu99wmydf0rk7l58f7</td>\n",
              "      <td>17/10/2024 18:10:59</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Parda</td>\n",
              "      <td>Não</td>\n",
              "      <td>Não acredito que minha experiência profissiona...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3ynsw7z0hl5hhpbfvaeqk73ynsw7z04l</td>\n",
              "      <td>22/10/2024 18:03:00</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Branca</td>\n",
              "      <td>Não</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v6bji2ct5xckpl1uouv6bjiotkrf3b4f</td>\n",
              "      <td>23/10/2024 01:33:23</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Branca</td>\n",
              "      <td>Não</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>jct4toasj61rygexnyug3zjct4toascu</td>\n",
              "      <td>25/10/2024 12:02:06</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Parda</td>\n",
              "      <td>Não</td>\n",
              "      <td>Não acredito que minha experiência profissiona...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>vcvc4otfqznwludyp3mvcvc8euvs299t</td>\n",
              "      <td>12/11/2024 19:45:44</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Parda</td>\n",
              "      <td>Sim</td>\n",
              "      <td>Não acredito que minha experiência profissiona...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7o0jgas7pedzh1dr17o0jgzt0irkbqo6</td>\n",
              "      <td>21/11/2024 05:13:09</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Parda</td>\n",
              "      <td>Não</td>\n",
              "      <td>Não acredito que minha experiência profissiona...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gmkbjmsg8fzfqljgmkbjmkr5zfkex28o</td>\n",
              "      <td>09/12/2024 12:16:43</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Parda</td>\n",
              "      <td>Não</td>\n",
              "      <td>Não acredito que minha experiência profissiona...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>vwsjeyi7044yittt3zdwvwsjefc53s6y</td>\n",
              "      <td>09/12/2024 14:03:12</td>\n",
              "      <td>18</td>\n",
              "      <td>17-21</td>\n",
              "      <td>Masculino</td>\n",
              "      <td>Parda</td>\n",
              "      <td>Não</td>\n",
              "      <td>Não acredito que minha experiência profissiona...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 403 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a370d57a-ec1d-4435-bfea-2343cee4ea55')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a370d57a-ec1d-4435-bfea-2343cee4ea55 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a370d57a-ec1d-4435-bfea-2343cee4ea55');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-580c639e-3e6c-4572-8556-7cd30546084f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-580c639e-3e6c-4572-8556-7cd30546084f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-580c639e-3e6c-4572-8556-7cd30546084f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Verificando os valores únicos em cada coluna\n",
        "for column in df.columns:\n",
        "    print(f\"Valores únicos na coluna '{column}':\")\n",
        "    print(df[column].unique())\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Contagem de valores únicos por coluna\n",
        "print(\"Contagem de valores únicos por coluna:\")\n",
        "print(df.nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vX6hDDPyV-2",
        "outputId": "75427a32-7c53-47b3-8dd3-27bbfb8b3c4e",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            " 'Dados georeferenciados, Dados relacionais (estruturados em bancos SQL)'\n",
            " 'Dados relacionais (estruturados em bancos SQL), Imagens'\n",
            " 'Google Analytics' 'Excel' 'Imagens'\n",
            " 'Dados relacionais (estruturados em bancos SQL), Amostras de equipamentos de telecomunicação'\n",
            " 'Vídeos, Dados relacionais (estruturados em bancos SQL)'\n",
            " 'Dados armazenados em bancos NoSQL, S3'\n",
            " 'Dados georeferenciados, Textos/Documentos'\n",
            " 'Dados relacionais (estruturados em bancos SQL), API'\n",
            " 'Planilhas, Datalake' 'Imagens, Textos/Documentos'\n",
            " 'Imagens, Dados georeferenciados'\n",
            " 'Dados relacionais (estruturados em bancos SQL), Arquivos'\n",
            " 'Dados georeferenciados, Planilhas'\n",
            " 'Imagens, Dados relacionais (estruturados em bancos SQL)' 'DataLake'\n",
            " 'Planilhas, Imagens' 'Imagens, Planilhas'\n",
            " 'Textos/Documentos, Dados armazenados em bancos NoSQL'\n",
            " 'Dados armazenados em bancos NoSQL, Api'\n",
            " 'Dados georeferenciados, Dados nao estruturados'\n",
            " 'Textos/Documentos, Dados georeferenciados'\n",
            " 'Dados georeferenciados, Imagens'\n",
            " 'Dados relacionais (estruturados em bancos SQL), APIs externas, Eventos JSON vindo de Kafka Brokers'\n",
            " 'Áudios'\n",
            " 'Dados relacionais (estruturados em bancos SQL), Plataforma de dados unificada'\n",
            " 'Dados relacionais (estruturados em bancos SQL), Vídeos'\n",
            " 'Dados armazenados em bancos NoSQL, Imagens'\n",
            " 'Dados relacionais (estruturados em bancos SQL), Third Party APIs'\n",
            " 'Dados relacionais (estruturados em bancos SQL), api' 'Planilhas, Denodo'\n",
            " 'Dados relacionais (estruturados em bancos SQL), APIs'\n",
            " 'Planilhas, Dashboard'\n",
            " 'Dados relacionais (estruturados em bancos SQL), Áudios'\n",
            " 'Não uso muitas fontes de dados'\n",
            " 'Dados relacionais (estruturados em bancos SQL), Dados relacionais (estruturados fora de bancos SQL)'\n",
            " 'Dados armazenados em bancos NoSQL, Dados georeferenciados'\n",
            " 'BD em Progress' 'Vídeos, Textos/Documentos' 'Planilhas, Erp'\n",
            " 'Todas as alternativas anteriores' 'Planilhas, Share point'\n",
            " 'Json, parquet' 'Dados relacionais (estruturados em bancos SQL), Kafka'\n",
            " 'Áudios, Textos/Documentos' 'Microdados' 'big data em lakehouse'\n",
            " 'Dados relacionais (estruturados em bancos SQL), SAP'\n",
            " 'Dados relacionais (estruturados em bancos SQL), SAP, Sales Force, dados do mercado financeiro para trades'\n",
            " 'Dados relacionais (estruturados em bancos SQL), JSON']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.1_Dados relacionais (estruturados em bancos SQL)':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.2_Dados armazenados em bancos NoSQL':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.3_Imagens':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.4_Textos/Documentos':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.5_Vídeos':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.6_Áudios':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.7_Planilhas':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.8_Dados georeferenciados':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d_linguagem_de_programacao_(dia_a_dia)':\n",
            "['Python, JavaScript, SQL' 'Python'\n",
            " 'Não utilizo linguagem de programação no trabalho' 'SQL'\n",
            " 'SQL, JavaScript' nan 'SQL, Python' 'Python, SQL'\n",
            " 'Python, SQL, Visual Basic/VBA' 'SQL, R, Python'\n",
            " 'SQL, Python, Visual Basic/VBA, PHP' 'SQL, Python, JavaScript'\n",
            " 'SQL, Python, PHP' 'SQL, Python, SAS/Stata, Visual Basic/VBA'\n",
            " 'Visual Basic/VBA' 'Visual Basic/VBA, SQL' 'SQL, Python, R, Scala'\n",
            " 'Python, R, SQL' 'SQL, Python, Scala' 'SQL, R'\n",
            " 'SQL, Não utilizo linguagem de programação no trabalho'\n",
            " 'JavaScript, Visual Basic/VBA'\n",
            " 'SQL, Python, JavaScript, Visual Basic/VBA' 'Python, Visual Basic/VBA'\n",
            " 'SQL, Python, SAS/Stata, Java' 'SQL, Python, Java'\n",
            " 'SQL, Python, PHP, JavaScript' 'SQL, PHP, Python' 'R'\n",
            " 'Python, SQL, SAS/Stata' 'SQL, C/C++/C#, Python, .NET' 'SQL, R, PHP'\n",
            " 'SQL, Python, Java, Rust, Scala' 'SQL, Python, R'\n",
            " 'SQL, Python, SAS/Stata' 'Python, SQL, Rust'\n",
            " 'Python, C/C++/C#, .NET, SQL' 'SQL, Python, Matlab, JavaScript'\n",
            " 'SQL, Python, C/C++/C#' 'R, SQL, Python' 'SQL, Python, Java, JavaScript'\n",
            " 'SQL, Python, R, Julia, Visual Basic/VBA'\n",
            " 'SQL, R, Python, Visual Basic/VBA, JavaScript'\n",
            " 'SQL, Visual Basic/VBA, Python, R' 'Python, R'\n",
            " 'SQL, R, Python, Visual Basic/VBA, Scala' 'R, Python' 'SQL, SAS/Stata'\n",
            " 'SQL, Python, .NET' 'SQL, R, Visual Basic/VBA'\n",
            " 'R, SQL, .NET, C/C++/C#, JavaScript' 'R, SQL' 'Python, JavaScript'\n",
            " 'Python, SQL, JavaScript' 'SQL, Python, Visual Basic/VBA'\n",
            " 'Python, SQL, JavaScript, R' 'SQL, Python, R, Visual Basic/VBA'\n",
            " 'SQL, Python, Java, Visual Basic/VBA' 'Scala'\n",
            " 'SQL, Python, JavaScript, PHP' 'SQL, Python, JavaScript, Matlab'\n",
            " '.NET, SQL' 'Python, SQL, R, Visual Basic/VBA' 'Scala, Python, SQL'\n",
            " 'Python, Scala' 'SQL, Python, Visual Basic/VBA, Scala, JavaScript'\n",
            " 'SQL, Visual Basic/VBA' 'SQL, Python, Java, Scala' 'SQL, PHP, JavaScript'\n",
            " 'Python, SQL, Visual Basic/VBA, JavaScript'\n",
            " 'SQL, R, Python, Visual Basic/VBA' 'SQL, R, Python, SAS/Stata'\n",
            " 'SQL, Python, Scala, Java' 'Python, SQL, R' 'SQL, Python, Matlab'\n",
            " 'R, Python, SQL' 'JavaScript' 'Visual Basic/VBA, Python'\n",
            " 'Python, SQL, Scala'\n",
            " 'Python, Não utilizo linguagem de programação no trabalho'\n",
            " 'Python, SQL, Java, Scala' 'Java, Python, JavaScript'\n",
            " 'SQL, Visual Basic/VBA, Python' 'Python, R, SQL, Visual Basic/VBA'\n",
            " 'SQL, JavaScript, Python' 'Python, Java' 'SQL, Python, Rust'\n",
            " 'SQL, Python, C/C++/C#, .NET' 'Python, C/C++/C#'\n",
            " 'Visual Basic/VBA, Python, SQL'\n",
            " 'Não utilizo linguagem de programação no trabalho, SQL, Python'\n",
            " 'SQL, SAS/Stata, Python' 'C/C++/C#' 'Python, Scala, SQL'\n",
            " 'SQL, .NET, Visual Basic/VBA' 'Python, SQL, Java'\n",
            " 'Visual Basic/VBA, C/C++/C#' 'Scala, SQL, Python' 'SQL, Python, Julia, R'\n",
            " 'SQL, Scala' 'SQL, Python, Visual Basic/VBA, JavaScript'\n",
            " 'Python, SQL, Matlab, PHP' 'SQL, SAS/Stata, R' '.NET'\n",
            " 'SQL, Python, Java, R' 'Python, Scala, JavaScript, SQL'\n",
            " 'SQL, Python, C/C++/C#, .NET, Java, Visual Basic/VBA'\n",
            " 'R, Python, SQL, JavaScript' 'SQL, R, Python, Java, Matlab'\n",
            " 'SQL, Python, JavaScript, C/C++/C#' 'SQL, Python, SAS/Stata, JavaScript'\n",
            " 'SQL, C/C++/C#, Visual Basic/VBA' 'R, Python, SQL, Visual Basic/VBA'\n",
            " 'Visual Basic/VBA, R, SQL' 'SQL, Python, Scala, SAS/Stata'\n",
            " 'SQL, Python, SAS/Stata, Scala' 'Python, Java, JavaScript'\n",
            " 'Python, SQL, C/C++/C#, .NET' 'SQL, PHP, JavaScript, Python' 'SAS/Stata'\n",
            " 'SQL, .NET, C/C++/C#, JavaScript' 'Python, SAS/Stata, SQL'\n",
            " 'SQL, R, Python, JavaScript' 'Python, SQL, C/C++/C#'\n",
            " 'Python, SQL, Scala, Visual Basic/VBA'\n",
            " 'SQL, R, Python, SAS/Stata, Visual Basic/VBA, Matlab, JavaScript'\n",
            " 'R, Python, Não utilizo linguagem de programação no trabalho'\n",
            " 'R, Visual Basic/VBA, SQL' 'SQL, R, Python, SAS/Stata, Visual Basic/VBA'\n",
            " 'Python, Scala, R' 'SQL, Python, SAS/Stata, Visual Basic/VBA, Matlab'\n",
            " 'SQL, C/C++/C#' 'SQL, Java, Python' 'SQL, R, Python, SAS/Stata, Scala'\n",
            " 'Python, Java, SQL' 'SQL, Python, R, Java'\n",
            " 'SQL, SAS/Stata, Visual Basic/VBA' 'SQL, Python, Visual Basic/VBA, R'\n",
            " 'SQL, R, Python, Julia' 'SQL, Python, SAS/Stata, R'\n",
            " 'SQL, Java, JavaScript, Python' 'JavaScript, SQL' 'Scala, SQL'\n",
            " 'SQL, R, Python, Rust' 'Visual Basic/VBA, SQL, Python'\n",
            " 'SQL, Python, Java, Scala, JavaScript'\n",
            " 'SQL, Python, C/C++/C#, Java, PHP, JavaScript, Scala'\n",
            " 'SQL, JavaScript, PHP' 'SQL, Scala, Python' 'SQL, Java, JavaScript'\n",
            " 'SQL, R, JavaScript' 'SQL, Python, Java, PHP'\n",
            " 'SQL, R, Visual Basic/VBA, Python'\n",
            " 'SQL, Python, Java, Scala, Rust, Visual Basic/VBA'\n",
            " 'SQL, R, Python, Matlab' 'Python, Rust' 'SQL, Python, JavaScript, Java'\n",
            " 'R, Python, SQL, Julia'\n",
            " 'SQL, R, Python, SAS/Stata, Visual Basic/VBA, Scala, JavaScript'\n",
            " 'SQL, .NET' 'Matlab, Python, SQL' 'R, Visual Basic/VBA'\n",
            " 'SQL, Python, Java, Rust'\n",
            " 'SQL, Python, Java, Visual Basic/VBA, JavaScript'\n",
            " 'SQL, Python, C/C++/C#, Scala'\n",
            " 'SQL, Python, C/C++/C#, SAS/Stata, Visual Basic/VBA, R'\n",
            " 'SQL, Python, .NET, Visual Basic/VBA' 'Python, SQL, JavaScript, Scala'\n",
            " 'SQL, R, SAS/Stata' 'R, SQL, SAS/Stata'\n",
            " 'SQL, Python, SAS/Stata, Scala, R, Visual Basic/VBA'\n",
            " 'Python, SQL, Java, JavaScript' 'PHP, JavaScript'\n",
            " 'Python, R, SQL, Matlab, JavaScript'\n",
            " 'SQL, Visual Basic/VBA, Python, SAS/Stata' 'Python, SQL, Matlab'\n",
            " 'SQL, R, Python, C/C++/C#, .NET, Rust, Scala' 'R, Python, SQL, Matlab'\n",
            " 'SQL, Python, Scala, Matlab' 'Scala, Python'\n",
            " 'SQL, Python, Matlab, Visual Basic/VBA'\n",
            " 'SQL, Python, C/C++/C#, SAS/Stata, JavaScript'\n",
            " 'SQL, R, SAS/Stata, Python' 'SQL, Python, R, Matlab'\n",
            " 'SQL, C/C++/C#, JavaScript, .NET, Python'\n",
            " 'SQL, Python, Java, SAS/Stata, Scala, JavaScript' 'SQL, R, Python, Java'\n",
            " 'SQL, Python, C/C++/C#, R, SAS/Stata, Scala, Java'\n",
            " 'Python, SQL, .NET, C/C++/C#, Java, Visual Basic/VBA, Scala, PHP, JavaScript'\n",
            " 'Java, Python' 'SQL, Python, Scala, PHP' 'SQL, Python, R, JavaScript'\n",
            " 'SQL, R, Python, JavaScript, Visual Basic/VBA'\n",
            " 'Python, JavaScript, SQL, Java' 'R, SQL, Python, Visual Basic/VBA'\n",
            " 'Scala, Java, SQL' 'SQL, Python, SAS/Stata, PHP'\n",
            " 'SQL, SAS/Stata, R, Python' 'SQL, Python, SAS/Stata, PHP, Java'\n",
            " 'SQL, Scala, Java' 'SQL, C/C++/C#, PHP, JavaScript' 'Python, Matlab'\n",
            " 'R, SAS/Stata' 'Python, .NET, JavaScript, SQL'\n",
            " 'SQL, Python, Java, SAS/Stata' 'SQL, Python, C/C++/C#, PHP'\n",
            " 'R, Python, SAS/Stata' 'PHP, SQL'\n",
            " 'SQL, Python, C/C++/C#, .NET, Java, Scala, JavaScript']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.1_SQL':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.2_R':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.3_Python':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.4_C/C++/C#':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.5_.NET':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.6_Java':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.7_Julia':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.8_SAS/Stata':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.9_Visual Basic/VBA':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.10_Scala':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.11_Matlab':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.12_Rust':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.13_PHP':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.14_JavaScript':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.15_Não utilizo nenhuma das linguagens listadas':\n",
            "[ 0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.e_linguagem_mais_usada':\n",
            "['Python' nan 'SQL' 'JavaScript' 'C/C++/C#' 'Visual Basic/VBA'\n",
            " 'Não utilizo nenhuma das linguagens listadas no trabalho' 'R' '.NET'\n",
            " 'Scala' 'SAS/Stata' 'Java' 'Julia' 'PHP' 'Matlab']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.f_linguagem_preferida':\n",
            "['Python' nan 'SQL' 'C/C++/C#' 'Scala' 'Rust' 'R' 'LookML' 'Javascript'\n",
            " 'nenhum' 'Sql' 'PySpark' '.' 'Julia' 'Engenharia de Prompt' 'Qliksense'\n",
            " 'Golang' 'PHP' 'Go' 'sql' 'Clojure' 'Não sei' 'Matlab' 'M/DAX' 'SPSS'\n",
            " 'GO' 'Pyspark' 'No code' 'SAS' 'M e Dax' 'sas' 'Spark'\n",
            " 'Não trabalharia com linguagem de programação' 'Databricks']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g_banco_de_dados_(dia_a_dia)':\n",
            "['Google BigQuery' 'Databricks' 'MySQL, CoachDB' ...\n",
            " 'Oracle, Elasticsearch, SQLite'\n",
            " 'DynamoDB, Amazon Athena, Amazon Redshift, S3' 'MySQL, PostgreSQL, Neo4J']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.1_MySQL':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.2_Oracle':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.3_SQL SERVER':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.4_Amazon Aurora ou RDS':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.5_DynamoDB':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.6_CoachDB':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.7_Cassandra':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.8_MongoDB':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.9_MariaDB':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.10_Datomic':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.11_S3':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.12_PostgreSQL':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.13_ElasticSearch':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.14_DB2':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.15_Microsoft Access':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.16_SQLite':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.17_Sybase':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.18_Firebase':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.19_Vertica':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.20_Redis':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.21_Neo4J':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.22_Google BigQuery':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.23_Google Firestore':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.24_Amazon Redshift':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.25_Amazon Athena':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.26_Snowflake':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.27_Databricks':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.28_HBase':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.29_Presto':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.30_Splunk':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.31_SAP HANA':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.32_Hive':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.33_Firebird':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h_cloud_(dia_a_dia)':\n",
            "['Servidores On Premise/Não utilizamos Cloud' 'Azure (Microsoft)'\n",
            " 'Google Cloud (GCP)' 'Amazon Web Services (AWS)' nan 'IBM'\n",
            " 'Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Cloud Própria'\n",
            " 'Oracle Cloud, Azure (Microsoft), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Google Cloud (GCP), Digital Ocean' 'Oracle Cloud'\n",
            " 'Amazon Web Services (AWS), Cloud Própria, Oracle Cloud'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Cloud Própria'\n",
            " 'Google Cloud (GCP), nenhuma'\n",
            " 'Amazon Web Services (AWS), Google Cloud (GCP)'\n",
            " 'Azure (Microsoft), IBM, Cloud Própria' 'IBM, Azure (Microsoft)'\n",
            " 'Cloud Própria, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Azure (Microsoft), Oracle Cloud'\n",
            " 'Google Cloud (GCP), Amazon Web Services (AWS), Azure (Microsoft)'\n",
            " 'Azure (Microsoft), Google Cloud (GCP), Amazon Web Services (AWS)'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Cloud Própria, Oracle Cloud'\n",
            " 'Google Cloud (GCP), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS)' 'Não utilizado Cloud'\n",
            " 'Azure (Microsoft), hdfs, impala' 'Google Drive'\n",
            " 'Google Cloud (GCP), Azure (Microsoft)'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Google Cloud (GCP)'\n",
            " 'Não utilizo' 'Não usamos cloud'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS)'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS), Oracle Cloud, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Cloud Própria, Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS)'\n",
            " 'Google Cloud (GCP), Amazon Web Services (AWS)'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Servidores On Premise/Não utilizamos Cloud, Google Cloud (GCP)'\n",
            " 'Nenhuma' 'Não usamos'\n",
            " 'Azure (Microsoft), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Amazon Web Services (AWS), Oracle Cloud'\n",
            " 'Amazon Web Services (AWS), Cloud Própria'\n",
            " 'Google Cloud (GCP), Oracle Cloud'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Azure (Microsoft)'\n",
            " 'Azure (Microsoft), IBM' 'Amazon Web Services (AWS), Databricks'\n",
            " 'Amazon Web Services (AWS), Google Cloud (GCP), Azure (Microsoft)'\n",
            " 'Azure (Microsoft), Oracle Cloud, Amazon Web Services (AWS)'\n",
            " 'Azure (Microsoft), Huawei'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft)'\n",
            " 'Cloud Própria, Azure (Microsoft)' 'Google Cloud (GCP), Magalu Cloud'\n",
            " 'TOTVS'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS), IBM, Oracle Cloud, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Heroku' 'Azure (Microsoft), Google Cloud (GCP)'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Azure (Microsoft), Cloud Própria'\n",
            " 'A empresa usa Oracle cloud, mas eu não tenho acesso ainda'\n",
            " 'Google Cloud (GCP), Azure (Microsoft), Amazon Web Services (AWS)'\n",
            " 'Amazon Web Services (AWS), Cloudera' 'Qlik sense' 'Não Utilizo'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Oracle Cloud'\n",
            " 'Cloud Própria, IBM, Azure (Microsoft)'\n",
            " 'Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud, Azure (Microsoft)'\n",
            " 'Protheus' 'Google Cloud (GCP), Oracle Cloud, Amazon Web Services (AWS)'\n",
            " 'Amazon Web Services (AWS), Sharepoint'\n",
            " 'Azure (Microsoft), Cloud Própria, Amazon Web Services (AWS)'\n",
            " 'Google Cloud (GCP), Azure (Microsoft), Oracle Cloud'\n",
            " 'Google Cloud (GCP), Cloud Própria' 'Google Cloud (GCP), Databricks'\n",
            " 'Azure (Microsoft), IBM, Google Cloud (GCP), Amazon Web Services (AWS)'\n",
            " 'Oracle Cloud, Cloud Própria'\n",
            " 'Oracle Cloud, Servidores On Premise/Não utilizamos Cloud' 'nenhum'\n",
            " 'IBM, Azure (Microsoft), Google Cloud (GCP)' 'sharepoint'\n",
            " 'Azure (Microsoft), Cloud Própria'\n",
            " 'Google Cloud (GCP), IBM, Amazon Web Services (AWS), Cloud Própria'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Azure (Microsoft), Openshift' 'Cloud Própria, Amazon Web Services (AWS)'\n",
            " 'Databricks'\n",
            " 'Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud, Google Cloud (GCP), Azure (Microsoft)'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS), IBM, Google Cloud (GCP), Azure (Microsoft)'\n",
            " 'não uso'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Google Cloud (GCP), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Qlik' 'Digital ocean'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS), Google Cloud (GCP)'\n",
            " 'Azure (Microsoft), IBM, Amazon Web Services (AWS)'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Google Cloud (GCP), Amazon Web Services (AWS)'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), IBM, Cloud Própria'\n",
            " 'Oracle Cloud, Amazon Web Services (AWS), Azure (Microsoft), Cloud Própria'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, IBM' 'HPCC System'\n",
            " 'Não utilizo.'\n",
            " 'Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud, IBM'\n",
            " 'ArcGIS Online'\n",
            " 'Google Cloud (GCP), Azure (Microsoft), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Azure (Microsoft), Google Cloud (GCP), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Amazon Web Services (AWS), Google Cloud (GCP), Oracle Cloud'\n",
            " 'Eu especificamente não uso cloud, mas a empresa azure' 'Nenhum'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Google Cloud (GCP)'\n",
            " 'SharePoint' 'Cloud Própria, Google Cloud (GCP)'\n",
            " 'Oracle Cloud, Azure (Microsoft)' 'Google Cloud (GCP), IBM'\n",
            " 'Cloud Própria, Amazon Web Services (AWS), Azure (Microsoft), Oracle Cloud'\n",
            " 'Não sei' 'Oracle Cloud, IBM, Cloudera'\n",
            " 'Oracle Cloud, Google Cloud (GCP)'\n",
            " 'Amazon Web Services (AWS), Google Cloud (GCP), Azure (Microsoft), Cloud Própria'\n",
            " 'IBM, Amazon Web Services (AWS)' 'Amazon Web Services (AWS), IBM'\n",
            " 'Azure (Microsoft), Nao' 'Azure (Microsoft), Oracle Cloud, Cloud Própria'\n",
            " 'Huawei Cloud'\n",
            " 'Amazon Web Services (AWS), Oracle Cloud, Azure (Microsoft)'\n",
            " 'Amazon Web Services (AWS), Google Cloud (GCP), Cloud Própria, Azure (Microsoft)'\n",
            " 'IBM, Cloud Própria'\n",
            " 'Google Cloud (GCP), Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS)'\n",
            " 'Azure (Microsoft), Oracle Cloud, Google Cloud (GCP)' 'nenhuma'\n",
            " 'Amazon Web Services (AWS), Posit'\n",
            " 'Amazon Web Services (AWS), clickhouse' 'Cloud Própria, Huawei Cloud'\n",
            " 'Azure (Microsoft), Google Cloud (GCP), Oracle Cloud, Amazon Web Services (AWS)'\n",
            " 'Digital Ocean'\n",
            " 'Oracle Cloud, Cloud Própria, Google Cloud (GCP), Amazon Web Services (AWS)'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Google Cloud (GCP), Oracle Cloud'\n",
            " '.' 'Servidores On Premise/Não utilizamos Cloud, Oracle Cloud'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Oracle Cloud, Amazon Web Services (AWS), Azure (Microsoft), Google Cloud (GCP)'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Azure (Microsoft), Amazon Web Services (AWS)'\n",
            " 'Oracle Cloud, Azure (Microsoft), Amazon Web Services (AWS)'\n",
            " 'Google Cloud (GCP), Amazon Web Services (AWS), Cloud Própria, Azure (Microsoft)'\n",
            " 'Google Cloud (GCP), Cloud Própria, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS), Denodo' 'Snowflake'\n",
            " 'Azure (Microsoft), Denodo' 'IBM, Dremio'\n",
            " 'Cloud Própria, Google Cloud (GCP), Oracle Cloud, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Oracle Cloud, Google Cloud (GCP)'\n",
            " 'Não uso'\n",
            " 'Azure (Microsoft), Oracle Cloud, Amazon Web Services (AWS), Google Cloud (GCP), Huawei'\n",
            " 'Google Cloud (GCP), Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS), Azure (Microsoft)'\n",
            " 'DataBricks'\n",
            " 'Azure (Microsoft), Cloud Própria, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Azure (Microsoft), Google Cloud (GCP)'\n",
            " 'Azure (Microsoft), Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS)'\n",
            " 'Denodo' 'IBM, Google Cloud (GCP), Oracle Cloud' 'Onedrive'\n",
            " 'Amazon Web Services (AWS), Oracle Cloud, Google Cloud (GCP), Azure (Microsoft)'\n",
            " 'Oracle Cloud, Amazon Web Services (AWS)' 'Desconhecido'\n",
            " 'Amazon Web Services (AWS), Oracle Cloud, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Algar'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Google Cloud (GCP), IBM'\n",
            " 'Azure (Microsoft), Cloud Própria, Google Cloud (GCP), Amazon Web Services (AWS)'\n",
            " 'Google Cloud (GCP), Protheus'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS), IBM'\n",
            " 'Amazon Web Services (AWS), Snowflake'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Oracle Cloud, Amazon Web Services (AWS)'\n",
            " 'IBM, Oracle Cloud, Azure (Microsoft)' 'Azure (Microsoft), GoodData'\n",
            " 'Empresa não usa cloud'\n",
            " 'Oracle Cloud, Azure (Microsoft), Google Cloud (GCP), Amazon Web Services (AWS)'\n",
            " 'Amazon Web Services (AWS), Oracle Cloud, Google Cloud (GCP)' 'SAP'\n",
            " 'Azure (Microsoft), Google Cloud (GCP), Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Google Cloud (GCP), Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Não utilizo atualmente'\n",
            " 'Oracle Cloud, Azure (Microsoft), Google Cloud (GCP)' 'Não utilizamos.'\n",
            " 'IBM, Azure (Microsoft), Oracle Cloud'\n",
            " 'Amazon Web Services (AWS), IBM, Google Cloud (GCP), Azure (Microsoft)'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, HostDime'\n",
            " 'Oracle Cloud, Amazon Web Services (AWS), Google Cloud (GCP), Azure (Microsoft)'\n",
            " 'Nao utilizamos cloud'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS), Oracle Cloud, Google Cloud (GCP)'\n",
            " 'Azure (Microsoft), Google Cloud (GCP), Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud, Oracle Cloud'\n",
            " 'Hostdime' 'Não utilizo Cloud no projeto' 'Não temos banco de dados'\n",
            " 'Azure (Microsoft), Datadricks'\n",
            " 'Oracle Cloud, Amazon Web Services (AWS), Google Cloud (GCP)'\n",
            " 'Azure (Microsoft), Servidores On Premise/Não utilizamos Cloud, Cloud Própria'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS), Cloud Própria'\n",
            " 'Google Cloud (GCP), Amazon Web Services (AWS), Magalu Cloud'\n",
            " 'Amazon Web Services (AWS), Azure (Microsoft), Cloud Própria'\n",
            " 'Azure (Microsoft), Amazon Web Services (AWS), Oracle Cloud'\n",
            " 'Azure (Microsoft), Servidores On Premise/Não utilizamos Cloud, IBM'\n",
            " 'Cloud Própria, Oracle Cloud, UOL Host'\n",
            " 'Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS), Cloudera'\n",
            " 'Sharepoint' 'Cloudera' 'Azure (Microsoft), OPENSHIFT'\n",
            " 'Azure (Microsoft), Oracle Cloud, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Amazon Web Services (AWS), Google Cloud (GCP), Cloud Própria, IBM, Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Amazon Web Services (AWS), Google Cloud (GCP), Servidores On Premise/Não utilizamos Cloud, Azure (Microsoft)'\n",
            " '.Nenhuma' 'Servidores On Premise/Não utilizamos Cloud, Cloud QlikSense'\n",
            " 'Google Cloud (GCP), Azure (Microsoft), Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud'\n",
            " 'Ambiente próprio'\n",
            " 'Google Cloud (GCP), Oracle Cloud, Amazon Web Services (AWS), Azure (Microsoft)'\n",
            " 'Azure (Microsoft), Cloudera' 'Azure (Microsoft), Digital Ocean'\n",
            " 'não utilizamos CLOUD'\n",
            " 'Não uso cloud nem banco de dados.  Uso cluster e HPC'\n",
            " 'Azure (Microsoft), Google Cloud (GCP), Amazon Web Services (AWS), Databricks'\n",
            " 'Amazon Web Services (AWS), Servidores On Premise/Não utilizamos Cloud, Google Cloud (GCP)'\n",
            " 'Amazon Web Services (AWS), Salesforce'\n",
            " 'Azure (Microsoft), Google Cloud (GCP), Amazon Web Services (AWS), Cloud Própria'\n",
            " 'Amazon Web Services (AWS), IBM, Google Cloud (GCP)' 'Teradata Vantage'\n",
            " 'Google Cloud (GCP), Azure (Microsoft), Amazon Web Services (AWS), Oracle Cloud'\n",
            " 'Não utilizo nuvem'\n",
            " 'Google Cloud (GCP), Cloud Própria, Servidores On Premise/Não utilizamos Cloud, Amazon Web Services (AWS)']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.1_Amazon Web Services (AWS)':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.2_Google Cloud (GCP)':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.3_Azure (Microsoft)':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.4_Oracle Cloud':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.5_IBM':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.6_Servidores On Premise/Não utilizamos Cloud':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.7_Cloud Própria':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.i_cloud_preferida':\n",
            "['Amazon Web Services (AWS)' 'Azure (Microsoft)' nan 'Google Cloud (GCP)'\n",
            " 'Não sei opinar / Não tenho preferência' 'Databricks' 'Oracle' 'nao uso'\n",
            " 'Qlik' 'Não sei' 'Cloud com Databricks' 'clickhouse' 'Datadricks'\n",
            " 'Teradata Vantage']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j_ferramenta_de_bi_(dia_a_dia)':\n",
            "['Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI' 'Redash' 'Metabase, Microsoft PowerBI, Grafana'\n",
            " 'Não utilizo nenhuma ferramenta de BI no trabalho' nan\n",
            " 'Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker'\n",
            " 'Grafana, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Metabase, Looker, Microsoft PowerBI' 'Qlik View/Qlik Sense'\n",
            " 'Outra opção' 'Amazon Quicksight'\n",
            " 'Metabase, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Pentaho'\n",
            " 'Microsoft PowerBI, Tableau' 'Superset'\n",
            " 'Grafana, Metabase, Microsoft PowerBI, Alteryx'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker, Metabase' 'Microsoft PowerBI, Grafana'\n",
            " 'Amazon Quicksight, Alteryx, SAS Visual Analytics, Tableau'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI, Looker'\n",
            " 'SAP Business Objects/SAP Analytics, Microsoft PowerBI'\n",
            " 'Looker, Tableau, Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Grafana'\n",
            " 'Microsoft PowerBI, Looker'\n",
            " 'Outra opção, Salesforce/Einstein Analytics, Tableau'\n",
            " 'SAP Business Objects/SAP Analytics'\n",
            " 'SAS Visual Analytics, Microsoft PowerBI, SAP Business Objects/SAP Analytics'\n",
            " 'Looker' 'Tableau, Microsoft PowerBI' 'Oracle Business Intelligence'\n",
            " 'Microsoft PowerBI, Metabase'\n",
            " 'Microsoft PowerBI, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, Pentaho, Alteryx, Tableau' 'Metabase'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Outra opção, Microsoft PowerBI' 'Tableau' 'SAS Visual Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção'\n",
            " 'Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, SAP Business Objects/SAP Analytics'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI'\n",
            " 'Qlik View/Qlik Sense, Tableau, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Tableau, Pentaho' 'Redash, Tableau, Superset'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Metabase'\n",
            " 'Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Superset, Microsoft PowerBI, Grafana' 'Grafana'\n",
            " 'Microsoft PowerBI, Superset' 'Metabase, Grafana, Microsoft PowerBI'\n",
            " 'Superset, Redash, Metabase, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Pentaho' 'Tableau, Amazon Quicksight' 'Metabase, Outra opção'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense, Outra opção'\n",
            " 'Grafana, Microsoft PowerBI, Redash'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'Metabase, Amazon Quicksight, Looker'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense, Looker, SAS Visual Analytics, Tableau'\n",
            " 'Grafana, Não utilizo nenhuma ferramenta de BI no trabalho, Looker, Oracle Business Intelligence'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense'\n",
            " 'Oracle Business Intelligence, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Tableau, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Looker, Tableau'\n",
            " 'Microsoft PowerBI, Tableau, Qlik View/Qlik Sense, Amazon Quicksight'\n",
            " 'Qlik View/Qlik Sense, Metabase, Microsoft PowerBI, Looker, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Looker Studio (antigo Google Data Studio), Tableau' 'Looker, Tableau'\n",
            " 'SAP Business Objects/SAP Analytics, Oracle Business Intelligence, Looker, Pentaho, Looker Studio (antigo Google Data Studio), Metabase, Alteryx, Microsoft PowerBI, Salesforce/Einstein Analytics'\n",
            " 'Alteryx, Tableau, Looker Studio (antigo Google Data Studio), Amazon Quicksight, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Tableau, Grafana'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Looker, Microsoft PowerBI, Metabase'\n",
            " 'Grafana, Looker, Metabase' 'Tableau, Outra opção'\n",
            " 'Microsoft PowerBI, Grafana, Metabase, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense, Redash'\n",
            " 'Tableau, Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Grafana, Outra opção, Pentaho'\n",
            " 'Microsoft PowerBI, Pentaho' 'Pentaho, Microsoft PowerBI'\n",
            " 'Alteryx, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'SAP Business Objects/SAP Analytics, Looker Studio (antigo Google Data Studio), Looker, Tableau'\n",
            " 'Metabase, Microsoft PowerBI' 'Microsoft PowerBI, Outra opção'\n",
            " 'Salesforce/Einstein Analytics, Microsoft PowerBI'\n",
            " 'Metabase, Looker Studio (antigo Google Data Studio), Looker'\n",
            " 'Amazon Quicksight, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Looker, Qlik View/Qlik Sense'\n",
            " 'Outra opção, Não utilizo nenhuma ferramenta de BI no trabalho'\n",
            " 'Pentaho, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, SAS Visual Analytics'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense, Pentaho'\n",
            " 'Pentaho, Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'Outra opção, Looker Studio (antigo Google Data Studio), Metabase'\n",
            " 'Grafana, Salesforce/Einstein Analytics, Metabase, Looker Studio (antigo Google Data Studio)'\n",
            " 'Superset, Metabase'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Grafana, Outra opção'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Oracle Business Intelligence, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Tableau, Outra opção, Amazon Quicksight'\n",
            " 'Superset, Pentaho, Microsoft PowerBI'\n",
            " 'Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Tableau, Oracle Business Intelligence'\n",
            " 'Looker, Microsoft PowerBI' 'Metabase, Superset'\n",
            " 'Metabase, Outra opção, Grafana'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Salesforce/Einstein Analytics, Outra opção'\n",
            " 'Looker, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Grafana, Tableau'\n",
            " 'Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI'\n",
            " 'Tableau, Grafana' 'Tableau, Amazon Quicksight, Alteryx'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Looker'\n",
            " 'Amazon Quicksight, Tableau'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense'\n",
            " 'Amazon Quicksight, Tableau, Microsoft PowerBI'\n",
            " 'Metabase, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, Metabase, Superset'\n",
            " 'SAS Visual Analytics, Microsoft PowerBI, Grafana'\n",
            " 'Looker, Amazon Quicksight, Outra opção'\n",
            " 'Microsoft PowerBI, Metabase, Looker Studio (antigo Google Data Studio)'\n",
            " 'Metabase, Looker, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Superset, Grafana' 'Qlik View/Qlik Sense, Metabase'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Não utilizo nenhuma ferramenta de BI no trabalho'\n",
            " 'Grafana, Microsoft PowerBI, Metabase'\n",
            " 'Grafana, Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio), Looker'\n",
            " 'Salesforce/Einstein Analytics, Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'Salesforce/Einstein Analytics, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Tableau, Qlik View/Qlik Sense' 'Metabase, Looker'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI, Tableau'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Looker'\n",
            " 'Microsoft PowerBI, Looker, Metabase'\n",
            " 'Metabase, Salesforce/Einstein Analytics, Outra opção'\n",
            " 'Amazon Quicksight, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Redash, Metabase, Grafana, Tableau'\n",
            " 'Outra opção, Tableau'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Superset, Pentaho, Metabase'\n",
            " 'Pentaho, Salesforce/Einstein Analytics, Microsoft PowerBI'\n",
            " 'Tableau, Looker, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker'\n",
            " 'Microsoft PowerBI, Alteryx' 'Superset, Metabase, Microsoft PowerBI'\n",
            " 'Grafana, Amazon Quicksight' 'Superset, Microsoft PowerBI'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Pentaho'\n",
            " 'Grafana, Looker, Amazon Quicksight, Microsoft PowerBI'\n",
            " 'Tableau, Amazon Quicksight, Microsoft PowerBI'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Tableau'\n",
            " 'Qlik View/Qlik Sense, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Grafana, Microsoft PowerBI, Looker, Tableau'\n",
            " 'Grafana, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Looker, Grafana, Microsoft PowerBI'\n",
            " 'Tableau, Microsoft PowerBI, Alteryx, Looker Studio (antigo Google Data Studio)'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, Tableau, Looker'\n",
            " 'Looker, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker Studio (antigo Google Data Studio), Metabase, Grafana'\n",
            " 'Superset, Qlik View/Qlik Sense'\n",
            " 'Microsoft PowerBI, Grafana, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Qlik View/Qlik Sense, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI'\n",
            " 'Tableau, Microsoft PowerBI, Qlik View/Qlik Sense, Looker'\n",
            " 'Salesforce/Einstein Analytics, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker, Superset, Metabase'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção'\n",
            " 'Qlik View/Qlik Sense, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção, Microsoft PowerBI'\n",
            " 'Outra opção, SAP Business Objects/SAP Analytics' 'Pentaho, Outra opção'\n",
            " 'Oracle Business Intelligence, Microsoft PowerBI'\n",
            " 'Metabase, Microsoft PowerBI, Looker, Superset, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense'\n",
            " 'Tableau, Looker, Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Redash'\n",
            " 'Tableau, Grafana, Outra opção' 'Tableau, Grafana, Metabase'\n",
            " 'Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção'\n",
            " 'Tableau, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Metabase, Looker, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Amazon Quicksight, Metabase, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Salesforce/Einstein Analytics, Looker Studio (antigo Google Data Studio)'\n",
            " 'Tableau, Outra opção, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Tableau, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Grafana, Tableau, Microsoft PowerBI'\n",
            " 'Pentaho, Grafana, Superset, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Salesforce/Einstein Analytics, Pentaho, Microsoft PowerBI, Qlik View/Qlik Sense'\n",
            " 'Pentaho, Metabase, Qlik View/Qlik Sense, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Grafana, SAP Business Objects/SAP Analytics, Microsoft PowerBI'\n",
            " 'Metabase, Tableau'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Microsoft PowerBI'\n",
            " 'Looker, Metabase, Qlik View/Qlik Sense, Microsoft PowerBI, Alteryx'\n",
            " 'Redash, Grafana' 'Microsoft PowerBI, Redash' 'Looker, Grafana'\n",
            " 'Looker, Tableau, Microsoft PowerBI' 'Looker, Amazon Quicksight, Tableau'\n",
            " 'Amazon QuickSight, Amazon Quicksight'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Outra opção, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Pentaho, Looker Studio (antigo Google Data Studio)'\n",
            " 'Tableau, Microsoft PowerBI, Qlik View/Qlik Sense, Looker Studio (antigo Google Data Studio), Looker, Alteryx'\n",
            " 'Microsoft PowerBI, Oracle Business Intelligence' 'Tableau, Looker'\n",
            " 'Microsoft PowerBI, Grafana, Outra opção'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Qlik View/Qlik Sense'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker, Grafana'\n",
            " 'SAS Visual Analytics, Microsoft PowerBI, Qlik View/Qlik Sense'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Grafana, Outra opção'\n",
            " 'Amazon Quicksight, Superset, Tableau, Grafana, Looker Studio (antigo Google Data Studio), Looker, Microsoft PowerBI, Metabase'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Outra opção, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Pentaho, Looker, Metabase'\n",
            " 'Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Alteryx, Amazon Quicksight, Tableau' 'Grafana, Tableau, Outra opção'\n",
            " 'Looker, Microsoft PowerBI, Tableau, Outra opção'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Outra opção'\n",
            " 'Looker, Microsoft PowerBI, Tableau'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Tableau'\n",
            " 'Salesforce/Einstein Analytics, Tableau, Microsoft PowerBI, Looker, Qlik View/Qlik Sense'\n",
            " 'Grafana, SAS Visual Analytics, Microsoft PowerBI, SAP Business Objects/SAP Analytics'\n",
            " 'Tableau, Amazon Quicksight, Metabase'\n",
            " 'Looker Studio (antigo Google Data Studio), Tableau, Looker'\n",
            " 'Superset, Grafana, Metabase, Looker'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Looker, Salesforce/Einstein Analytics, Tableau'\n",
            " 'Não utilizo nenhuma ferramenta de BI no trabalho, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Metabase, Microsoft PowerBI, Looker'\n",
            " 'SAS Visual Analytics, Tableau, Redash, Microsoft PowerBI, Grafana'\n",
            " 'Looker, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Tableau, Looker, Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'SAS Visual Analytics, Microsoft PowerBI, Pentaho'\n",
            " 'Alteryx, Metabase, Microsoft PowerBI'\n",
            " 'Qlik View/Qlik Sense, Tableau, Metabase'\n",
            " 'Grafana, Microsoft PowerBI, Outra opção'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker Studio (antigo Google Data Studio), Amazon Quicksight'\n",
            " 'Superset, Tableau, Microsoft PowerBI, Grafana'\n",
            " 'Amazon Quicksight, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Amazon Quicksight'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Grafana, Tableau'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Outra opção'\n",
            " 'Salesforce/Einstein Analytics, SAS Visual Analytics, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker Studio (antigo Google Data Studio), Metabase'\n",
            " 'Tableau, Microsoft PowerBI, Looker'\n",
            " 'Metabase, Amazon Quicksight, Microsoft PowerBI'\n",
            " 'Superset, Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Alteryx, Tableau'\n",
            " 'Amazon QuickSight, Metabase, Outra opção'\n",
            " 'Tableau, Looker, Microsoft PowerBI, Outra opção'\n",
            " 'Metabase, Looker Studio (antigo Google Data Studio)' 'Tableau, Metabase'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Amazon Quicksight'\n",
            " 'Grafana, Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'Outra opção, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Superset, Tableau'\n",
            " 'Looker Studio (antigo Google Data Studio), Alteryx, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Outra opção, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Metabase, Não utilizo nenhuma ferramenta de BI no trabalho'\n",
            " 'Grafana, Superset, Microsoft PowerBI, Outra opção'\n",
            " 'Looker Studio (antigo Google Data Studio), Tableau, Redash'\n",
            " 'Looker, Metabase, Alteryx, Microsoft PowerBI, Grafana, Qlik View/Qlik Sense, Tableau, Superset, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense, Looker, Grafana, Superset, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), SAP Business Objects/SAP Analytics, Looker'\n",
            " 'Pentaho, Looker, Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Metabase'\n",
            " 'SAP Business Objects/SAP Analytics, Qlik View/Qlik Sense, Microsoft PowerBI, Salesforce/Einstein Analytics, Pentaho'\n",
            " 'Looker, Alteryx, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Superset, Metabase, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Grafana, Qlik View/Qlik Sense, Tableau, Microsoft PowerBI'\n",
            " 'Outra opção, Looker' 'Pentaho, Qlik View/Qlik Sense, Microsoft PowerBI'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Amazon Quicksight, Looker Studio (antigo Google Data Studio)'\n",
            " 'SAP Business Objects/SAP Analytics, Microsoft PowerBI, Tableau'\n",
            " 'Amazon Quicksight, Outra opção'\n",
            " 'Looker Studio (antigo Google Data Studio), Grafana, Tableau, Looker'\n",
            " 'Looker, Redash, Microsoft PowerBI, Amazon Quicksight, Metabase, Superset, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Outra opção, SAS Visual Analytics, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Tableau, Microsoft PowerBI, Amazon Quicksight'\n",
            " 'Grafana, Metabase, Looker'\n",
            " 'Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker'\n",
            " 'Microsoft PowerBI, SAP Business Objects/SAP Analytics, Grafana, Salesforce/Einstein Analytics'\n",
            " 'Alteryx, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Looker, Tableau'\n",
            " 'Qlik View/Qlik Sense, Grafana, Looker Studio (antigo Google Data Studio), Looker, Superset'\n",
            " 'Redash, Microsoft PowerBI, Grafana, Tableau'\n",
            " 'Looker, Grafana, Superset, Looker Studio (antigo Google Data Studio)'\n",
            " 'Grafana, Salesforce/Einstein Analytics, Microsoft PowerBI'\n",
            " 'Amazon Quicksight, SAP Business Objects/SAP Analytics, Microsoft PowerBI, SAS Visual Analytics, Salesforce/Einstein Analytics, Grafana, Oracle Business Intelligence, Alteryx, Tableau'\n",
            " 'Amazon Quicksight, Metabase'\n",
            " 'Não utilizo nenhuma ferramenta de BI no trabalho, Microsoft PowerBI'\n",
            " 'SAS Visual Analytics, Microsoft PowerBI, Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Pentaho, Grafana, Microsoft PowerBI, Outra opção, Metabase, Tableau'\n",
            " 'Superset, Tableau, Outra opção' 'Grafana, Metabase'\n",
            " 'Alteryx, Salesforce/Einstein Analytics, SAP Business Objects/SAP Analytics, Microsoft PowerBI, Qlik View/Qlik Sense'\n",
            " 'Amazon Quicksight, Tableau, Alteryx'\n",
            " 'Tableau, Redash, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Grafana, Metabase'\n",
            " 'Microsoft PowerBI, Metabase, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Grafana, Tableau, Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'Metabase, Qlik View/Qlik Sense, Tableau, Alteryx, Microsoft PowerBI'\n",
            " 'SAS Visual Analytics, Amazon Quicksight, Tableau, Alteryx, Microsoft PowerBI'\n",
            " 'Grafana, Superset, Redash'\n",
            " 'Metabase, Looker, Microsoft PowerBI, Superset'\n",
            " 'Microsoft PowerBI, Pentaho, Superset'\n",
            " 'Alteryx, Microsoft PowerBI, Pentaho'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, SAS Visual Analytics, Microsoft PowerBI'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Amazon Quicksight'\n",
            " 'Looker Studio (antigo Google Data Studio), Grafana'\n",
            " 'Grafana, Looker, Outra opção'\n",
            " 'SAP Business Objects/SAP Analytics, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Metabase, Grafana, Salesforce/Einstein Analytics, Tableau, Microsoft PowerBI, Outra opção'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Outra opção'\n",
            " 'SAP Business Objects/SAP Analytics, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Microsoft PowerBI'\n",
            " 'Tableau, Looker, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'SAS Visual Analytics, Microsoft PowerBI'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Metabase, Tableau'\n",
            " 'Superset, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Looker Studio (antigo Google Data Studio), Superset'\n",
            " 'Redash, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker, Microsoft PowerBI, Tableau'\n",
            " 'Grafana, Superset, Metabase'\n",
            " 'Microsoft PowerBI, Looker, Alteryx, Qlik View/Qlik Sense, Looker Studio (antigo Google Data Studio)'\n",
            " 'Grafana, Looker Studio (antigo Google Data Studio)'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI, Salesforce/Einstein Analytics'\n",
            " 'SAS Visual Analytics, Amazon Quicksight'\n",
            " 'Microsoft PowerBI, Salesforce/Einstein Analytics, Grafana, Tableau'\n",
            " 'Microsoft PowerBI, SAS Visual Analytics, SAP Business Objects/SAP Analytics'\n",
            " 'Amazon Quicksight, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Pentaho, Metabase, Microsoft PowerBI, Grafana'\n",
            " 'Superset, Looker, Grafana'\n",
            " 'Superset, Grafana, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Redash'\n",
            " 'Metabase, Grafana'\n",
            " 'Metabase, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Metabase, Pentaho'\n",
            " 'Microsoft PowerBI, Metabase, Grafana'\n",
            " 'Metabase, Alteryx, Amazon Quicksight'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Superset'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Amazon QuickSight, Amazon Quicksight, Alteryx'\n",
            " 'Amazon QuickSight, Amazon Quicksight, Looker Studio (antigo Google Data Studio)'\n",
            " 'Tableau, Metabase, Looker Studio (antigo Google Data Studio)'\n",
            " 'Grafana, Redash'\n",
            " 'Metabase, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker, Outra opção'\n",
            " 'Metabase, Looker, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Outra opção'\n",
            " 'Tableau, Microsoft PowerBI, Qlik View/Qlik Sense'\n",
            " 'Microsoft PowerBI, Não utilizo nenhuma ferramenta de BI no trabalho'\n",
            " 'Tableau, Pentaho, Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Amazon Quicksight, Tableau'\n",
            " 'Microsoft PowerBI, Alteryx, Pentaho'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI, Looker, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Oracle Business Intelligence, Microsoft PowerBI, SAP Business Objects/SAP Analytics, Salesforce/Einstein Analytics, SAS Visual Analytics'\n",
            " 'Grafana, Looker' 'Metabase, Redash'\n",
            " 'Grafana, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção'\n",
            " 'Looker, Redash, Metabase, Grafana' 'Tableau, Grafana, Microsoft PowerBI'\n",
            " 'Looker Studio (antigo Google Data Studio), Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Amazon Quicksight, Grafana, Microsoft PowerBI'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Tableau, Outra opção'\n",
            " 'Não utilizo nenhuma ferramenta de BI no trabalho, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção'\n",
            " 'Grafana, Microsoft PowerBI, SAP Business Objects/SAP Analytics'\n",
            " 'Alteryx'\n",
            " 'Metabase, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Metabase'\n",
            " 'Looker, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Tableau, Alteryx'\n",
            " 'Salesforce/Einstein Analytics, Grafana, Microsoft PowerBI'\n",
            " 'Looker, Metabase, Grafana' 'Tableau, Superset, Redash'\n",
            " 'Looker, Qlik View/Qlik Sense, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Amazon Quicksight, Tableau'\n",
            " 'Alteryx, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Qlik View/Qlik Sense'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Outra opção'\n",
            " 'Microsoft PowerBI, Não utilizo nenhuma ferramenta de BI no trabalho, Outra opção'\n",
            " 'Pentaho, Grafana, Looker Studio (antigo Google Data Studio), Looker'\n",
            " 'Pentaho, Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'Pentaho, Looker Studio (antigo Google Data Studio), Looker, Grafana'\n",
            " 'Tableau, Pentaho'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Pentaho, Microsoft PowerBI, Outra opção'\n",
            " 'Looker Studio (antigo Google Data Studio), Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker, Microsoft PowerBI, Metabase'\n",
            " 'Looker Studio (antigo Google Data Studio), Tableau, Grafana, Outra opção'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'SAS Visual Analytics, Microsoft PowerBI, Tableau'\n",
            " 'Tableau, Looker, Grafana, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, SAS Visual Analytics, Outra opção, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker, Salesforce/Einstein Analytics, Looker Studio (antigo Google Data Studio)'\n",
            " 'Amazon Quicksight, Pentaho'\n",
            " 'Microsoft PowerBI, Tableau, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Grafana, Metabase, Tableau, Outra opção' 'Grafana, Tableau, Looker'\n",
            " 'Tableau, Microsoft PowerBI, Looker, Superset'\n",
            " 'Looker, Tableau, Metabase, Looker Studio (antigo Google Data Studio)'\n",
            " 'Tableau, Pentaho, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Grafana, Tableau'\n",
            " 'Microsoft PowerBI, Salesforce/Einstein Analytics, Looker'\n",
            " 'Tableau, Microsoft PowerBI, Outra opção'\n",
            " 'Looker, Metabase, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Metabase, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Tableau, Microsoft PowerBI, Qlik View/Qlik Sense, Salesforce/Einstein Analytics, Outra opção'\n",
            " 'Metabase, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense, Grafana, Looker'\n",
            " 'Metabase, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Grafana, Salesforce/Einstein Analytics'\n",
            " 'Superset, Looker Studio (antigo Google Data Studio), Amazon Quicksight, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Grafana, Alteryx, SAS Visual Analytics, Looker, Oracle Business Intelligence, Salesforce/Einstein Analytics, Redash, Qlik View/Qlik Sense'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Metabase, Salesforce/Einstein Analytics, Tableau, Looker'\n",
            " 'Grafana, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Alteryx, Tableau, Microsoft PowerBI'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Redash, Amazon Quicksight, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Tableau, Redash, Superset'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Looker, Grafana, Tableau'\n",
            " 'Salesforce/Einstein Analytics, Pentaho, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Outra opção, Grafana'\n",
            " 'Tableau, Superset, Metabase, Microsoft PowerBI'\n",
            " 'Tableau, Alteryx, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Pentaho, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker, Pentaho, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Grafana, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Looker Studio (antigo Google Data Studio), Grafana, Tableau, Outra opção'\n",
            " 'Looker, Tableau, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio), Looker, Tableau'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Alteryx, Microsoft PowerBI'\n",
            " 'Oracle Business Intelligence, SAS Visual Analytics, Salesforce/Einstein Analytics, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI'\n",
            " 'Redash, Looker, Amazon Quicksight, Looker Studio (antigo Google Data Studio), Grafana, Superset, Tableau, Metabase, Microsoft PowerBI'\n",
            " 'Grafana, Alteryx, Tableau, Amazon Quicksight'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, Amazon Quicksight, Metabase'\n",
            " 'Redash, Metabase, Looker, Looker Studio (antigo Google Data Studio), Grafana'\n",
            " 'Superset, Looker Studio (antigo Google Data Studio)'\n",
            " 'Metabase, Microsoft PowerBI, Outra opção, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Metabase, Tableau, Looker'\n",
            " 'Looker, Grafana, Redash, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense, SAP Business Objects/SAP Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Looker'\n",
            " 'Microsoft PowerBI, Salesforce/Einstein Analytics, Metabase'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker, SAS Visual Analytics, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'Grafana, Looker Studio (antigo Google Data Studio), Redash, Superset, Microsoft PowerBI, Amazon Quicksight, Metabase'\n",
            " 'Tableau, Oracle Business Intelligence'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Metabase, Salesforce/Einstein Analytics'\n",
            " 'Redash, Superset'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'SAP Business Objects/SAP Analytics, Tableau, Microsoft PowerBI'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI, Metabase'\n",
            " 'Metabase, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Grafana, Microsoft PowerBI, Qlik View/Qlik Sense, Pentaho, Tableau'\n",
            " 'Metabase, Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'Metabase, Amazon Quicksight'\n",
            " 'Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker'\n",
            " 'Qlik View/Qlik Sense, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Qlik View/Qlik Sense, Amazon Quicksight'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Outra opção, Grafana'\n",
            " 'Superset, Looker, Metabase'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Superset'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense, Microsoft PowerBI, SAS Visual Analytics, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Microsoft PowerBI, Amazon Quicksight'\n",
            " 'Superset, Grafana, Microsoft PowerBI'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Pentaho, Metabase'\n",
            " 'Metabase, Tableau, Outra opção'\n",
            " 'Microsoft PowerBI, Tableau, Outra opção' 'Redash, SAS Visual Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Redash'\n",
            " 'Tableau, Looker, Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense'\n",
            " 'Salesforce/Einstein Analytics, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Metabase'\n",
            " 'Tableau, Amazon Quicksight, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Amazon Quicksight, Grafana'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Tableau, Looker'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, SAS Visual Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Metabase, Looker'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Metabase, Microsoft PowerBI'\n",
            " 'Alteryx, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Pentaho, Looker'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Microsoft PowerBI, Amazon Quicksight'\n",
            " 'Grafana, Superset'\n",
            " 'Qlik View/Qlik Sense, Tableau, Metabase, Superset, Grafana'\n",
            " 'Pentaho, Oracle Business Intelligence, Tableau, SAP Business Objects/SAP Analytics, Microsoft PowerBI, Metabase'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Alteryx, Pentaho, SAP Business Objects/SAP Analytics'\n",
            " 'Microsoft PowerBI, Grafana, Metabase'\n",
            " 'Looker, Redash, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Qlik View/Qlik Sense, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Tableau, Looker, Looker Studio (antigo Google Data Studio)'\n",
            " 'Looker Studio (antigo Google Data Studio), Tableau, Looker, Grafana'\n",
            " 'Microsoft PowerBI, SAS Visual Analytics, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker'\n",
            " 'Salesforce/Einstein Analytics, Tableau'\n",
            " 'Microsoft PowerBI, Salesforce/Einstein Analytics, Outra opção'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Tableau, Qlik View/Qlik Sense'\n",
            " 'Grafana, Looker, Superset'\n",
            " 'Grafana, Qlik View/Qlik Sense, Microsoft PowerBI, Metabase'\n",
            " 'Microsoft PowerBI, Pentaho, SAS Visual Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), SAP Business Objects/SAP Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Pentaho, Metabase'\n",
            " 'Amazon Quicksight, Microsoft PowerBI, Qlik View/Qlik Sense'\n",
            " 'Pentaho, Outra opção, Metabase'\n",
            " 'Tableau, Salesforce/Einstein Analytics, Microsoft PowerBI, Grafana, Redash'\n",
            " 'SAP Business Objects/SAP Analytics, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Metabase, Looker Studio (antigo Google Data Studio), Amazon Quicksight, Qlik View/Qlik Sense, Tableau, Microsoft PowerBI, Alteryx, Salesforce/Einstein Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense, Microsoft PowerBI'\n",
            " 'Tableau, Microsoft PowerBI, Salesforce/Einstein Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Looker, Grafana'\n",
            " 'Alteryx, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Metabase, Looker Studio (antigo Google Data Studio), Alteryx, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Metabase, Salesforce/Einstein Analytics, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Microsoft PowerBI, SAS Visual Analytics, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, Oracle Business Intelligence, Superset'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Microsoft PowerBI, Tableau, Pentaho, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Salesforce/Einstein Analytics, Qlik View/Qlik Sense, Oracle Business Intelligence, SAP Business Objects/SAP Analytics'\n",
            " 'Amazon Quicksight, Alteryx' 'Tableau, Alteryx'\n",
            " 'Looker, Tableau, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Qlik View/Qlik Sense, Grafana'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Tableau, Qlik View/Qlik Sense'\n",
            " 'Microsoft PowerBI, Metabase, Looker, Qlik View/Qlik Sense'\n",
            " 'Qlik View/Qlik Sense, Oracle Business Intelligence'\n",
            " 'Qlik View/Qlik Sense, Microsoft PowerBI, Pentaho'\n",
            " 'Looker, Metabase, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Grafana, Amazon Quicksight, Superset'\n",
            " 'Metabase, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Superset'\n",
            " 'Looker Studio (antigo Google Data Studio), Pentaho, Amazon Quicksight, Microsoft PowerBI, Tableau'\n",
            " 'Não utilizo nenhuma ferramenta de BI no trabalho, Outra opção'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio), Tableau, Grafana'\n",
            " 'Microsoft PowerBI, Looker, Tableau, Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense, Metabase'\n",
            " 'Looker, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio), Tableau'\n",
            " 'Microsoft PowerBI, Metabase, Looker'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Amazon Quicksight, Tableau, Looker, Redash, Metabase'\n",
            " 'Tableau, Amazon Quicksight, SAS Visual Analytics'\n",
            " 'Salesforce/Einstein Analytics, Microsoft PowerBI, Metabase'\n",
            " 'Grafana, Metabase, Microsoft PowerBI' 'Looker, Amazon Quicksight'\n",
            " 'Tableau, Microsoft PowerBI, Metabase, Superset'\n",
            " 'Microsoft PowerBI, Grafana, Tableau, Outra opção'\n",
            " 'Amazon Quicksight, Superset, Redash, Grafana' 'Alteryx, Tableau'\n",
            " 'Tableau, Metabase, Pentaho' 'Alteryx, Tableau, Amazon Quicksight'\n",
            " 'Microsoft PowerBI, Metabase, Tableau, Looker'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção'\n",
            " 'Tableau, SAS Visual Analytics, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Grafana'\n",
            " 'Microsoft PowerBI, SAS Visual Analytics, Qlik View/Qlik Sense'\n",
            " 'Grafana, Looker, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker'\n",
            " 'Looker Studio (antigo Google Data Studio), Metabase, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Metabase'\n",
            " 'Microsoft PowerBI, Alteryx, SAP Business Objects/SAP Analytics'\n",
            " 'Looker, Tableau, SAP Business Objects/SAP Analytics'\n",
            " 'Tableau, Pentaho, Microsoft PowerBI, Qlik View/Qlik Sense'\n",
            " 'Microsoft PowerBI, Superset, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Looker Studio (antigo Google Data Studio), Metabase, Microsoft PowerBI, Redash'\n",
            " 'Looker, Metabase, Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Metabase, Looker, Tableau, Microsoft PowerBI, Looker Studio (antigo Google Data Studio)'\n",
            " 'Grafana, Outra opção, Tableau'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Qlik View/Qlik Sense, Salesforce/Einstein Analytics'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Oracle Business Intelligence, Microsoft PowerBI'\n",
            " 'Tableau, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Pentaho'\n",
            " 'Tableau, Microsoft PowerBI, Redash'\n",
            " 'Looker Studio (antigo Google Data Studio), Alteryx, Tableau'\n",
            " 'Alteryx, Looker, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Superset, Tableau, Looker Studio (antigo Google Data Studio)'\n",
            " 'Redash, Amazon Quicksight, Grafana, Superset'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Grafana'\n",
            " 'Looker Studio (antigo Google Data Studio), Metabase, Looker'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Grafana, Looker, Tableau, Pentaho, Redash, Amazon Quicksight, Oracle Business Intelligence'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Amazon Quicksight'\n",
            " 'Microsoft PowerBI, Looker, Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense'\n",
            " 'Outra opção, Superset' 'Looker, Grafana, Tableau'\n",
            " 'Redash, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Metabase, Microsoft PowerBI, Grafana, Salesforce/Einstein Analytics'\n",
            " 'Redash, Grafana, SAS Visual Analytics, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção, Tableau'\n",
            " 'Grafana, Salesforce/Einstein Analytics, Redash'\n",
            " 'Pentaho, Microsoft PowerBI, Amazon Quicksight'\n",
            " 'SAS Visual Analytics, Looker'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Looker Studio (antigo Google Data Studio)'\n",
            " 'Qlik View/Qlik Sense, Pentaho, Grafana'\n",
            " 'Pentaho, Microsoft PowerBI, Looker'\n",
            " 'Metabase, Grafana, Tableau, Outra opção'\n",
            " 'Microsoft PowerBI, Looker, Salesforce/Einstein Analytics, Qlik View/Qlik Sense, Tableau'\n",
            " 'Looker, Looker Studio (antigo Google Data Studio), Tableau, Grafana'\n",
            " 'Tableau, Salesforce/Einstein Analytics'\n",
            " 'Oracle Business Intelligence, Microsoft PowerBI, Tableau, Pentaho, Qlik View/Qlik Sense'\n",
            " 'Microsoft PowerBI, Pentaho, Grafana, Metabase, Qlik View/Qlik Sense, Tableau'\n",
            " 'Microsoft PowerBI, Metabase, Tableau'\n",
            " 'Metabase, Microsoft PowerBI, Amazon Quicksight'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Amazon Quicksight, Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Metabase'\n",
            " 'Metabase, Superset, Pentaho'\n",
            " 'Grafana, Redash, Looker Studio (antigo Google Data Studio)'\n",
            " 'Microsoft PowerBI, Pentaho, SAS Visual Analytics, Tableau, Redash, Grafana'\n",
            " 'Grafana, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Outra opção'\n",
            " 'Redash, Microsoft PowerBI, SAP Business Objects/SAP Analytics, Grafana, Oracle Business Intelligence, Salesforce/Einstein Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Qlik View/Qlik Sense, Looker, Microsoft PowerBI, Tableau'\n",
            " 'Microsoft PowerBI, Tableau, Salesforce/Einstein Analytics'\n",
            " 'Tableau, Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Metabase'\n",
            " 'Tableau, Qlik View/Qlik Sense' 'Looker, Qlik View/Qlik Sense'\n",
            " 'Microsoft PowerBI, Pentaho, Outra opção'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Outra opção'\n",
            " 'Amazon Quicksight, Microsoft PowerBI, Alteryx, Tableau, Grafana'\n",
            " 'Pentaho, Superset, Qlik View/Qlik Sense, Grafana, Microsoft PowerBI'\n",
            " 'Redash, Amazon Quicksight, Looker'\n",
            " 'Metabase, Microsoft PowerBI, Pentaho, Outra opção'\n",
            " 'Alteryx, Microsoft PowerBI, Tableau'\n",
            " 'Microsoft PowerBI, Tableau, Salesforce/Einstein Analytics, Alteryx'\n",
            " 'Looker Studio (antigo Google Data Studio), Microsoft PowerBI, Metabase'\n",
            " 'Microsoft PowerBI, Pentaho, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Metabase, Redash, Looker Studio (antigo Google Data Studio)'\n",
            " 'SAS Visual Analytics, Tableau, Oracle Business Intelligence, Redash, Grafana, Microsoft PowerBI'\n",
            " 'Qlik View/Qlik Sense, Outra opção' 'Redash, Looker, Microsoft PowerBI'\n",
            " 'Microsoft PowerBI, Salesforce/Einstein Analytics, Tableau, SAS Visual Analytics, Oracle Business Intelligence'\n",
            " 'Microsoft PowerBI, Amazon Quicksight, Looker'\n",
            " 'Tableau, Amazon Quicksight, Microsoft PowerBI, Alteryx'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, Outra opção'\n",
            " 'Pentaho, Tableau, Microsoft PowerBI'\n",
            " 'Tableau, Looker Studio (antigo Google Data Studio), Superset'\n",
            " 'Tableau, Oracle Business Intelligence, Alteryx'\n",
            " 'Microsoft PowerBI, Grafana, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Oracle Business Intelligence'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Oracle Business Intelligence, Grafana'\n",
            " 'Alteryx, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Tableau, Microsoft PowerBI, Grafana'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Alteryx, Grafana, Outra opção'\n",
            " 'Looker Studio (antigo Google Data Studio), Grafana, Microsoft PowerBI, Tableau, Looker, Amazon Quicksight'\n",
            " 'Microsoft PowerBI, Qlik View/Qlik Sense, SAS Visual Analytics, Alteryx'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Microsoft PowerBI, SAP Business Objects/SAP Analytics'\n",
            " 'Microsoft PowerBI, Looker Studio (antigo Google Data Studio), Pentaho'\n",
            " 'Tableau, SAP Business Objects/SAP Analytics, SAS Visual Analytics'\n",
            " 'Tableau, Amazon Quicksight, Alteryx, Qlik View/Qlik Sense, Microsoft PowerBI, SAS Visual Analytics'\n",
            " 'Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Salesforce/Einstein Analytics'\n",
            " 'Looker Studio (antigo Google Data Studio), Tableau, Salesforce/Einstein Analytics, Looker'\n",
            " 'Tableau, Salesforce/Einstein Analytics, Looker Studio (antigo Google Data Studio), Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Pentaho, SAS Visual Analytics, Salesforce/Einstein Analytics'\n",
            " 'Microsoft PowerBI, SAS Visual Analytics, Tableau'\n",
            " 'Fazemos todas as análises utilizando apenas Excel ou planilhas do google, Grafana, SAS Visual Analytics'\n",
            " 'Looker, Redash, Microsoft PowerBI, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'Tableau, SAP Business Objects/SAP Analytics'\n",
            " 'Tableau, Microsoft PowerBI, Grafana, Amazon Quicksight'\n",
            " 'Tableau, Alteryx, Qlik View/Qlik Sense'\n",
            " 'SAP Business Objects/SAP Analytics, Grafana, Microsoft PowerBI, Salesforce/Einstein Analytics, Tableau, SAS Visual Analytics, Fazemos todas as análises utilizando apenas Excel ou planilhas do google'\n",
            " 'SAP Business Objects/SAP Analytics, Qlik View/Qlik Sense, Microsoft PowerBI']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.1_Microsoft PowerBI':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.2_Qlik View/Qlik Sense':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.3_Tableau':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.4_Metabase':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.5_Superset':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.6_Redash':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.7_Looker':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.8_Looker Studio(Google Data Studio)':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.9_Amazon Quicksight':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.10_Alteryx':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.11_SAP Business Objects/SAP Analytics':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.12_Oracle Business Intelligence':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.13_Salesforce/Einstein Analytics':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.14_SAS Visual Analytics':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.15_Grafana':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.16_Pentaho':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.17_Fazemos todas as análises utilizando apenas Excel ou planilhas do google':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.18_Não utilizo nenhuma ferramenta de BI no trabalho':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.k_ferramenta_de_bi_preferida':\n",
            "['Microsoft PowerBI' 'Metabase' 'Não tenho preferência / Não sei opinar'\n",
            " nan 'Looker' 'Tableau' 'Amazon Quicksight' 'Qlik View/Qlik Sense'\n",
            " 'Apache Superset' 'pentaho' 'Superset' 'Thoughtspot' 'HEX' 'Shiny'\n",
            " 'Databricks' 'superset' 'Plotly Dash' 'Sisense'\n",
            " 'Databricks Business Intelligence'\n",
            " 'Dashboards em python com framework Dash' 'shiny' 'TS' 'Looker Studio'\n",
            " 'ThoughtSpot' 'Observable Framework' 'Shiny (web framework)'\n",
            " 'ChatGPT ou Vector Search' 'databricks'\n",
            " 'Self-service, não gosto de trabalhar com visualização' 'Streamlit'\n",
            " 'Alteryx' 'Nenhuma/ Não uso' 'Hex Tech' 'hex' 'Databricks SQL'\n",
            " 'DATABRICKS' 'T' 'Databricks Dashboard' 'Plotly' 'Markdown' 'Python'\n",
            " 'Databricks Dashboards' 'redash' 'HorusBI' 'Nenhuma' 'domo' 'Thoughspot'\n",
            " 'Sigma' 'Cube.dev' 'Count' 'Grafana' 'DASH & PLOTLY' 'ORACLE OBIEE'\n",
            " 'Knime' 'Databricks com opção de BI' 'DataStage' 'Spotfire' 'Hex'\n",
            " 'Jupyter' 'Python dash' 'SAS' 'MicroStrategy' 'Solução própria'\n",
            " 'Zoho Analytics']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l_tipo_de_uso_de_ai_generativa_e_llm_na_empresa':\n",
            "['Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Não sei opinar.' nan\n",
            " 'Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Não sei opinar.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Não sei opinar.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Não sei opinar.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Não sei opinar., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Não sei opinar., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Não sei opinar., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Não sei opinar., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Não sei opinar.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Não sei opinar.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual)'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc)'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Não tenho visto soluções de IA Generatica e LLMs sendo tradadas como prioridade pela empresa e pessoas, os poucos casos de uso são isolados ou ainda estão muito no início.'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Soluções de IA Generatica e LLMs estão sendo tratadas como principal frente do negócio (com o objetivo de substituir o modelo de negócio atual), Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes)., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Não sei opinar.'\n",
            " 'Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).'\n",
            " 'Equipes de desenvolvimento utilizando soluções no estilo Copilot (Github Copilot, AWS CodeWhisperer etc) para obter mais produtividade no processo de desenvolvimento., Colaboradores utilizando soluções baseadas em AI Generativa (como o ChatGPT por exemplo) de forma independente, com o objetivo de melhorar sua produtividade no dia a dia, sem um direcionamento centralizado., Existe um direcionamento centralizado para que colaboradores utilizem soluções baseadas em AI Generativa (como o ChatGPT por exemplo), incluindo apoio nos custos das ferramentas., Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de propor melhorias e inovações para impulsionar a diferenciação de produtos oferecidos para os clientes finais (exemplo: novos recursos, produtos, serviços etc), Uma ou mais equipes testando e aplicando soluções de AI Generativa e LLMs, com o objetivo de aumentar a eficiencia de processos internos (como produtividade das equipes internas, ou melhoria de processos existentes).']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.1 Colaboradores usando AI generativa de forma independente e descentralizada':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.2 Direcionamento centralizado do uso de AI generativa':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.3 Desenvolvedores utilizando Copilots':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.4 AI Generativa e LLMs para melhorar produtos externos para os clientes finais':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.5 AI Generativa e LLMs para melhorar produtos internos para os colaboradores':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.6 IA Generativa e LLMs como principal frente do negócio':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.7 IA Generativa e LLMs não é prioridade':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.8 Não sei opinar sobre o uso de IA Generativa e LLMs na empresa':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m_usa_chatgpt_ou_copilot_no_trabalho?':\n",
            "['Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia.'\n",
            " nan\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solução.'\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso., Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia.'\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso.'\n",
            " 'Não utilizo nenhum tipo de solução de IA Generativa para melhorar a produtividade no dia a dia.'\n",
            " 'Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia.'\n",
            " 'Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia., Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia.'\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solução., Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia.'\n",
            " 'Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia., Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia.'\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solução., Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia.'\n",
            " 'Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia., Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso.'\n",
            " 'Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia., Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solução.'\n",
            " 'Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia., Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso.'\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso., Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solução.'\n",
            " 'Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia., Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solução.'\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso., Não utilizo nenhum tipo de solução de IA Generativa para melhorar a produtividade no dia a dia.'\n",
            " 'Não utilizo nenhum tipo de solução de IA Generativa para melhorar a produtividade no dia a dia., Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia.'\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso., Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia.'\n",
            " 'Utilizo apenas soluções gratuitas (como por exemplo o ChatGPT), para me ajudar a ser mais produtivo no dia a dia., Não utilizo nenhum tipo de solução de IA Generativa para melhorar a produtividade no dia a dia.'\n",
            " 'Não utilizo nenhum tipo de solução de IA Generativa para melhorar a produtividade no dia a dia., Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso.'\n",
            " 'Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solução., Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e pago do meu próprio bolso.'\n",
            " 'Não utilizo nenhum tipo de solução de IA Generativa para melhorar a produtividade no dia a dia., Utilizo soluções pagas de AI Generativa (como por exemplo ChatGPT plus, MidJourney etc) e a empresa em que trabalho paga pela solução.'\n",
            " 'Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia., Não utilizo nenhum tipo de solução de IA Generativa para melhorar a produtividade no dia a dia.'\n",
            " 'Não utilizo nenhum tipo de solução de IA Generativa para melhorar a produtividade no dia a dia., Utilizo soluções no estilo \"Copilot\" (exemplo: Github Copilot, Amazon CodeWhisperer ou ChatGPT Plus) para ter mais produtividade no dia a dia.']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.1 Não uso soluções de AI Generativa com foco em produtividade':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.2 Uso soluções gratuitas de AI Generativa com foco em produtividade':\n",
            "[ 1. nan  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.3 Uso e pago pelas soluções de AI Generativa com foco em produtividade':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.4 A empresa que trabalho paga pelas soluções de AI Generativa com foco em produtividade':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.5 Uso soluções do tipo Copilot':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '5.a_objetivo_na_area_de_dados':\n",
            "[nan\n",
            " 'Primeiro emprego: Estou tentando encontrar a primeira oportunidade na área de dados'\n",
            " 'Migração de carreira: Trabalho em outra área e busco recolocação na área de dados'\n",
            " 'Preparação profissional: Estou buscando conhecimentos técnicos para no futuro assumir algum cargo na área de dados'\n",
            " 'Apenas conhecimentos: Busco conhecimentos em dados para utilizar na minha área de atuação (não trabalho na área de dados)'\n",
            " 'Criar soluções de inteligência artificial' 'Estou na area' 'Recolocacão'\n",
            " 'Continuar a carreira' 'Avançar na área de pesquisa'\n",
            " 'Me desenvolver profissionalmente na área, podendo transitar em diversas funções e me tornar especialista ou líder em determinada função/cargo'\n",
            " 'Aprimorar técnica, mesmo já trabalhando na área.' 'Pesquisa'\n",
            " 'Já trabalho com dados'\n",
            " 'Me aprofundar na área para fortalecer minha atuação profissional'\n",
            " 'trabalho normal'\n",
            " 'Trabalho com pesquisa acadêmica e tenho uma consultoria na área de dados.'\n",
            " 'Sou professora. Preciso dessa área para ensinar e produzir artigos/pesquisas/projetos.'\n",
            " 'Me aprofundar mais em dados e suas tecnologias' 'Empregado'\n",
            " 'Trabalho na área faz 6 anos e devo seguir nela.' 'Já sou da área'\n",
            " 'Estou buscando recolocação na área na qual já trabalhava.'\n",
            " 'Minha profissão já trabalho com dados'\n",
            " 'Já trabalhei como engenheiro de dados' 'Crescer na área'\n",
            " 'Promover impacto social'\n",
            " 'Trabalho na área de dados e busco recolocação na área de dados.'\n",
            " 'Aperfeiçoamento de carreira (já trabalho na área)'\n",
            " 'já trabalho com dados' 'Manter o trabalho na área de dados'\n",
            " 'Ter conhecimento mais sólido para impactar o fluxo de dados de ponta a ponta'\n",
            " 'Já atuo na área de dados' 'Aprimoramento profissional'\n",
            " 'Aprimorar meus conhecimentos em dados.'\n",
            " 'Pausa na carreira e buscando Cargo senior em dados'\n",
            " 'Melhorar meus produtos digitais' 'transição para engenharia'\n",
            " 'Já atuo há muitos anos na área de dados, na interface com pesquisa em geociências'\n",
            " 'Analista sênior de Goveranança de dados'\n",
            " 'Ampliar e aplicar os conh cimentos que já possuo com o meu tempo de atuação na área.'\n",
            " 'recolocação' 'sou analista de dados e programador backend'\n",
            " 'Me manter atualizado' 'tomada de decisao'\n",
            " 'Gestão de Times e Projetos de Dados'\n",
            " 'Sou aposentada e busco atualização para recolocação profissional.']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '5.b_oportunidade_buscada':\n",
            "[nan 'Engenheiro de Machine Learning/ML Engineer'\n",
            " 'Cientista de Dados/Data Scientist'\n",
            " 'Engenheiro de Dados/Arquiteto de Dados/Data Engineer/Data Architect'\n",
            " 'Analista de Dados/Data Analyst' 'Analista de Negócios/Business Analyst'\n",
            " 'Data Product Manager' 'Analista de BI/BI Analyst' 'Analytics Engineer'\n",
            " 'Devops, MLops, Cientista de Dados']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '5.c_tempo_em_busca_de_oportunidade':\n",
            "[nan '7 meses - 1 ano' '0 - 6 meses' '1 ano - 2 anos' 'acima de 2 anos']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '5.d_experiencia_em_processos_seletivos':\n",
            "[nan 'Ainda não me candidatei a nenhuma vaga na área'\n",
            " 'Já me candidatei, mas nunca fui chamado para entrevistas'\n",
            " 'Já participei de 1 a 3 entrevistas, mas não fui contratado'\n",
            " 'Participei de entrevistas e fui contratada'\n",
            " 'Participei de mais de 3 entrevistas, mas não fui contratado'\n",
            " 'Gostaria de migrar internamente na empresa em que trabalho como SWE, mais de um nível de gestão já sabe dessa intenção.'\n",
            " 'Readequando LinkedIn e Currículo'\n",
            " 'Me candidatei mas nunca chamaram para entrevista.'\n",
            " 'Não tenho formação e não tenho experiencia direta na area']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a_rotina_como_de':\n",
            "[nan\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\"\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " 'Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " 'Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\"\n",
            " \"Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " 'Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuido da qualidade dos dados, metadados e dicionário de dados., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " \"Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuido da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuido da qualidade dos dados, metadados e dicionário de dados.\"]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.1_Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.2_Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.3_Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.4_Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.5_Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.6_Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.7_Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.8_Cuido da qualidade dos dados, metadados e dicionário de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.9_Nenhuma das opções listadas refletem meu dia a dia.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b_ferramentas_etl_de':\n",
            "[nan 'Scripts Python' 'Scripts Python, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks'\n",
            " 'Apache Airflow, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Google Dataflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow'\n",
            " 'Apache Airflow, Scripts Python, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures' '.'\n",
            " 'Apache Airflow, Scripts Python, SQL & Stored Procedures, spark, hadoop, hdfs, impala'\n",
            " 'Scripts Python, SQL & Stored Procedures, SAP BW ETL, Databricks'\n",
            " 'Qlik Sense, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Qlik Sense, Databricks'\n",
            " 'SQL & Stored Procedures, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, AWS Glue, Databricks'\n",
            " 'Scripts Python, Apache Airflow'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache NiFi, AWS Glue, Apache Kafka'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Databricks, Apache Kafka'\n",
            " 'Scripts Python, Apache Airflow, Google Dataflow, SQL & Stored Procedures'\n",
            " 'Scripts Python, Databricks'\n",
            " 'SQL & Stored Procedures, Scripts Python, AWS Glue, Databricks, SQL Server Integration Services (SSIS), Oracle Data Integrator'\n",
            " 'SQL & Stored Procedures, Apache Airflow'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, SAP BW ETL, Databricks, Apache PySpark'\n",
            " 'AWS Glue, Pentaho' 'Não utilizo ferramentas de ETL'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Databricks'\n",
            " 'AWS Glue, SQL Server Integration Services (SSIS), Apache Airflow, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Databricks, DBT (DataBuildTool)'\n",
            " 'Alteryx' 'Scripts Python, AWS Glue, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, AWS Glue'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS), DBT'\n",
            " 'Databricks, Scripts Python, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Databricks'\n",
            " 'SQL Server Integration Services (SSIS), SQL & Stored Procedures'\n",
            " 'Scripts Python, Pentaho, Databricks' 'Oracle Data Integrator'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Databricks, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, dbt'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Fivetran, Snowflake?'\n",
            " 'Databricks'\n",
            " 'Apache Airflow, AWS Glue, Scripts Python, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Scripts Python'\n",
            " 'Scripts Python, Apache Airflow, AWS Glue'\n",
            " 'Apache Airflow, Google Dataflow'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, Databricks'\n",
            " 'Scripts Python, AWS Glue, Apache Airflow, SQL & Stored Procedures'\n",
            " 'Scripts Python, Oracle Data Integrator, Databricks'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, Qlik Sense'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Databricks, Scripts Python'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, Oracle Data Integrator'\n",
            " 'Apache Airflow, Scripts Python, SQL & Stored Procedures, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Google Dataflow'\n",
            " 'Apache Airflow, Scripts Python, Google Dataflow'\n",
            " 'Scripts Python, Google Dataflow, Apache Bean'\n",
            " 'Scripts Python, Apache Airflow, Google Dataflow'\n",
            " 'Apache Airflow, AWS Glue, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Dagster'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, DBT'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Apache Airflow, Scripts Python, AWS Glue'\n",
            " 'SQL & Stored Procedures, Fabric' 'Scripts Python, AWS Glue, Prefect'\n",
            " 'Scripts Python, Apache Airflow, Fivetran, Databricks'\n",
            " 'ADF e spark scala' 'Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, Fivetran, Google Dataflow, SAS Data Integration, Databricks, Meltano'\n",
            " 'Apache Airflow, Scripts Python, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue'\n",
            " 'Apache Airflow, SQL & Stored Procedures' 'AWS Glue, Dataiku'\n",
            " 'SQL & Stored Procedures, Pentaho, APIPass'\n",
            " 'Scripts Python, Não utilizo ferramentas de ETL'\n",
            " 'Scripts Python, Google Dataflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, Apache Airflow, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Databricks, Apache Flink'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Google Data Fusion'\n",
            " 'SQL & Stored Procedures, Scripts Python, dbt'\n",
            " 'Scripts Python, SQL & Stored Procedures, Empresa tem ferramenta própria low-code, similar ao NiFi'\n",
            " 'Scripts Python, AWS Glue'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, Google Dataflow, Stitch, Databricks'\n",
            " 'Apache NiFi, Scripts Python, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, Azure Synapse Analytics'\n",
            " 'SQL & Stored Procedures, Fivetran, Databricks'\n",
            " 'Scripts Python, AWS Glue, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache NiFi, Pentaho, SQL Server Integration Services (SSIS)'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Pentaho, Scripts Python'\n",
            " 'Pentaho, Databricks, SQL & Stored Procedures, Apache Airflow, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Fivetran'\n",
            " 'Apache Airflow, Scripts Python, AWS Glue, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS), Apache Airflow'\n",
            " 'Scripts Python, Apache Airflow, Apache NiFi, SQL & Stored Procedures, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, Databricks'\n",
            " 'Apache Airflow, SQL & Stored Procedures, Google Dataflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Talend'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS), SAS Data Integration, Databricks'\n",
            " 'SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, Google Dataflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, Knime'\n",
            " 'Apache Airflow, Scripts Python, SQL & Stored Procedures, Google Dataflow'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, AWS Glue, Databricks'\n",
            " 'Scripts Python, Databricks, Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, AWS Glue, Databricks'\n",
            " 'SQL & Stored Procedures, Scripts Python, Databricks'\n",
            " 'Scripts Python, Apache Airflow, Pentaho'\n",
            " 'Apache Airflow, AWS Glue, Apache NiFi, Scripts Python, SQL & Stored Procedures, Databricks, Alteryx'\n",
            " 'Scripts Python, Apache Airflow, Fivetran'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Apache Airflow, Stitch'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Databricks, Oracle Data Integrator'\n",
            " 'SQL & Stored Procedures, Scripts Python, Apache Airflow, Google Dataflow'\n",
            " 'Luigi' 'Scripts Python, SQL & Stored Procedures, Pentaho, Databricks'\n",
            " 'Scripts Python, Apache Airflow, Fivetran, Google Dataflow'\n",
            " 'SQL & Stored Procedures, AWS Glue, Oracle Data Integrator, SAS Data Integration'\n",
            " 'SQL & Stored Procedures, Databricks, dbt'\n",
            " 'AWS Glue, SQL & Stored Procedures, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, AWS Glue, Databricks, Não utilizo ferramentas de ETL'\n",
            " 'Apache Airflow, Apache NiFi, Databricks' 'Databricks, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Pentaho, Databricks'\n",
            " 'Scripts Python, Databricks, SQL & Stored Procedures'\n",
            " 'Apache Airflow, AWS Glue, Scripts Python'\n",
            " 'Google Dataflow, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Meltano'\n",
            " 'Databricks, Scripts Python, Azure Data Factory'\n",
            " 'Scripts Python, SQL Server Integration Services (SSIS), Pentaho'\n",
            " 'Scripts Python, SQL & Stored Procedures, Talend, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, SQL Server Integration Services (SSIS)'\n",
            " 'SQL & Stored Procedures, AWS Glue, SAS Data Integration, SQL Server Integration Services (SSIS), Knime'\n",
            " 'Apache Airflow, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, airbyte, dbt'\n",
            " 'Apache Airflow, Scripts Python, DBT'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Oracle Data Integrator, Databricks'\n",
            " 'AWS Glue, Databricks, Spark' 'AWS Glue'\n",
            " 'SQL & Stored Procedures, Scripts Python, Google Dataflow'\n",
            " 'Scripts Python, Apache Airflow, Pentaho, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Google Dataflow, Databricks'\n",
            " 'SQL & Stored Procedures, Scripts Python, Apache Airflow, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, AWS Glue, Alteryx, Google Dataflow, Oracle Data Integrator, SAP BW ETL, SAS Data Integration, Databricks, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, Apache Airflow, AWS Glue, Fivetran, SQL & Stored Procedures'\n",
            " 'Scripts Python, Apache Airflow, Databricks, Apache NiFi'\n",
            " 'Apache Airflow, Scripts Python, AWS Glue, Databricks'\n",
            " 'AWS Glue, Scripts Python, IBM DataStage'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, SQL Server Integration Services (SSIS), Azure Data Factory'\n",
            " 'Apache Airflow, SQL & Stored Procedures, Scripts Python'\n",
            " 'SQL & Stored Procedures, Rundeck'\n",
            " 'SQL & Stored Procedures, Scripts Python, Ferramenta proprietária'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, AWS Glue'\n",
            " 'Scripts Python, Apache Airflow, AWS Glue, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, dbt, dlt, airbyte'\n",
            " 'Apache Airflow, SQL & Stored Procedures, Google Dataflow, Databricks'\n",
            " 'Apache Airflow, AWS Glue, Scripts Python, Google Dataflow'\n",
            " 'Apache Airflow, Apache NiFi, AWS Glue, Scripts Python, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Fivetran, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS)'\n",
            " 'Databricks, dbt'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS), Databricks, Qlik Sense'\n",
            " 'Scripts Python, SQL & Stored Procedures, Google Dataflow, Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache NiFi, AWS Glue, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, AWS Glue, Pentaho, Google Dataflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Alteryx'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Airbyte'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, SQL Server Integration Services (SSIS), Knime'\n",
            " 'Databricks, Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Pentaho'\n",
            " 'Databricks, Pentaho' 'Apache Airflow, Databricks, Dbt'\n",
            " 'AWS Glue, Apache Airflow, Scripts Python, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, Talend, Pentaho'\n",
            " 'Apache Airflow, AWS Glue, SQL & Stored Procedures, Scripts Python, Dbt'\n",
            " 'Scripts Python, AWS Glue, Spark'\n",
            " 'SQL & Stored Procedures, Apache Airflow, AWS Glue'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Pentaho, Oracle Data Integrator, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Dataform'\n",
            " 'Scripts Python, AWS Glue, SQL & Stored Procedures, AMAZON EMR'\n",
            " 'Scripts Python, Databricks, AWS Glue'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, RedPanda'\n",
            " 'SQL & Stored Procedures, Databricks, SAS Data Integration'\n",
            " 'Scripts Python, SQL & Stored Procedures, Oracle Data Integrator, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Stitch, Google Dataflow'\n",
            " 'Databricks, Internal Open Source Data Platform'\n",
            " 'Scripts Python, SQL & Stored Procedures, Fivetran, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, AWS Lambda'\n",
            " 'Scripts Python, SQL & Stored Procedures, Oracle Data Integrator, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, Synapse'\n",
            " 'SQL & Stored Procedures, Azure Data Factory'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Scripts Python, AWS Glue, Componente proprietário'\n",
            " 'Scripts Python, Apache Airflow, IBM DataStage'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Fivetran, Stitch, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Talend, Apache Airflow'\n",
            " 'Scripts Python, Oracle Data Integrator'\n",
            " 'Scripts Python, SQL & Stored Procedures, Oracle Data Integrator, SAS Data Integration, Databricks, Pentaho'\n",
            " 'Google Dataflow' 'SQL & Stored Procedures, Airbyte'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS), SAS Data Integration'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Fivetran'\n",
            " 'Apache Airflow, AWS Glue' 'Qlik Sense, Databricks'\n",
            " 'Databricks, Apache Airflow, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, Oracle Data Integrator, Qlik Sense, Knime'\n",
            " 'Scripts Python, Google Dataflow, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Dagster e dbt'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, Databricks, data Factory, Power query'\n",
            " 'Scripts Python, Apache Airflow, AWS Glue, Pentaho, SQL & Stored Procedures, IBM DataStage, Databricks'\n",
            " 'SQL & Stored Procedures, SAP Data Services'\n",
            " 'Scripts Python, Apache Airflow, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, Fivetran, Databricks'\n",
            " 'Scripts Python, Apache Airflow, AWS Glue, Databricks'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, Pentaho'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Pentaho, Databricks, SQL Server Integration Services (SSIS)'\n",
            " 'SQL & Stored Procedures, Pentaho' 'SQL & Stored Procedures, DBT' 'dbt'\n",
            " 'Scripts Python, SQL & Stored Procedures, DBT, Airbyte'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, Pentaho, Oracle Data Integrator'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Databricks, Apache NiFi'\n",
            " 'SQL & Stored Procedures, Cloud Connect'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, Data Factory'\n",
            " 'Scripts Python, Google Dataflow, Apache Airflow'\n",
            " 'Scripts Python, SQL Server Integration Services (SSIS), SQL & Stored Procedures, Data Factory'\n",
            " 'Pentaho, Scripts Python' 'Spark'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho, Apache Hop'\n",
            " 'Scripts Python, AWS Glue, Oracle Data Integrator'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, AWS Glue, Talend, Pentaho, Alteryx, Oracle Data Integrator, SQL Server Integration Services (SSIS), Qlik Sense, Knime, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Databricks, Azure Synapse'\n",
            " 'SQL & Stored Procedures, Scripts Python, Apache Airflow, AWS Glue'\n",
            " 'Databricks, SAS Data Integration'\n",
            " 'SQL & Stored Procedures, Sap Datasphere / Hana'\n",
            " 'Apache Airflow, Datastream, airbyte'\n",
            " 'Knime, Alteryx, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, Apache NiFi, Google Dataflow, Databricks'\n",
            " 'Apache Airflow, Databricks, Azure Data Factory'\n",
            " 'Scripts Python, SQL & Stored Procedures, Talend'\n",
            " 'Scripts Python, SQL & Stored Procedures, Talend, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'SQL & Stored Procedures, AWS Glue'\n",
            " 'Apache NiFi, Apache Airflow, SQL & Stored Procedures, Scripts Python, SAP BW ETL, Qlik Sense'\n",
            " 'Databricks, SAS Guide' 'Dataflow by Meli'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho, Qlik Sense'\n",
            " 'SQL Server Integration Services (SSIS), SQL & Stored Procedures, Power Query'\n",
            " 'Scripts Python, SQL & Stored Procedures, Notebooks do Fabric'\n",
            " 'Scripts Python, Apache Airflow, Apache NiFi, AWS Glue, Talend, Oracle Data Integrator, Google Dataflow, Databricks, SAP BW ETL'\n",
            " 'Scripts Python, Fivetran'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Google Dataflow, Airbyte'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Apache Airflow'\n",
            " 'Scripts Python, AWS Glue, IBM DataStage, SQL Server Integration Services (SSIS), Apache Airflow, Ferramenta própria'\n",
            " 'SQL & Stored Procedures, Boomi'\n",
            " 'Scripts Python, Apache Airflow, Google Dataflow, IBM DataStage, Databricks'\n",
            " 'Databricks, SAS' 'SQL & Stored Procedures, IBM DataStage'\n",
            " 'SQL & Stored Procedures, SAP BW ETL'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, Knime, SAS Data Integration'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, AWS Glue'\n",
            " 'Oracle Data Integrator, Databricks'\n",
            " 'SQL & Stored Procedures, IBM DataStage, Pentaho'\n",
            " 'Scripts Python, SQL & Stored Procedures, Google Dataflow, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache NiFi, Pentaho, Qlik Sense'\n",
            " 'Databricks, SQL Server Integration Services (SSIS), Scripts Python, SQL & Stored Procedures'\n",
            " 'Scripts Python, Apache Airflow, Azure Synapse Pipelines'\n",
            " 'Oracle Data Integrator, Scripts Python, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Databricks, SQL Server Integration Services (SSIS), Microsoft Azure Data Factory'\n",
            " 'Apache NiFi, Scripts Python, SQL & Stored Procedures, IBM DataStage, Databricks'\n",
            " 'SQL & Stored Procedures, Scripts Python, Pentaho, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, AWS DMS'\n",
            " 'Scripts Python, SQL & Stored Procedures, SAS Data Integration, Databricks'\n",
            " 'Apache Airflow, Scripts Python, Databricks, DBT'\n",
            " 'Scripts Python, Stitch, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho, SAS Data Integration, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Alteryx'\n",
            " 'SQL & Stored Procedures, AWS Glue, Scripts Python, Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Oracle Data Integrator, Databricks, SQL Server Integration Services (SSIS), Pentaho'\n",
            " 'IBM DataStage, Scripts Python, Apache NiFi' 'Apache NiFi, AWS Glue'\n",
            " 'Apache NiFi, Apache Airflow, SQL & Stored Procedures, Scripts Python, AWS Glue, Databricks'\n",
            " 'Scripts Python, Apache Airflow, Apache NiFi, Pentaho, Databricks'\n",
            " 'SQL & Stored Procedures, AWS Glue, Google Dataflow'\n",
            " 'Databricks, Alteryx'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS), Scripts Python, Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, AWS Glue, PySpark'\n",
            " 'Scripts Python, Apache Airflow, Alteryx, Apache NiFi, Databricks'\n",
            " 'Scripts Python, Apache NiFi'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Fivetran, SAS Data Integration, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Pentaho, SQL Server Integration Services (SSIS), Qlik Sense, Databricks'\n",
            " 'Scripts Python, Apache Airflow, Pentaho, Apache NiFi'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, SQL Server Integration Services (SSIS), Qlik Sense'\n",
            " 'Scripts Python, Google Dataflow, Pentaho, Talend, AWS Glue, Apache NiFi, Databricks, Apache Airflow, SQL Server Integration Services (SSIS)'\n",
            " 'AWS Glue, Databricks, DMS' 'Customizado'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Google Dataflow, SQL Server Integration Services (SSIS), Qlik Sense'\n",
            " 'Apache Airflow, Google Dataflow, Scripts Python'\n",
            " 'SQL & Stored Procedures, Alteryx, SAP BW ETL' 'Scripts Python, Pentaho'\n",
            " 'Scripts Python, SQL & Stored Procedures, Alteryx, Knime'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Stitch'\n",
            " 'SQL & Stored Procedures, Oracle Data Integrator, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, Azure Synapse'\n",
            " 'Scripts Python, Pentaho, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Apache NiFi, AWS Glue, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Scripts Python, AWS Glue, Databricks'\n",
            " 'AWS Glue, Scripts Python, Alteryx, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache NiFi, Databricks, Knime'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Pentaho, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, AWS Glue, Pentaho, SQL Server Integration Services (SSIS)'\n",
            " 'SQL & Stored Procedures, Apache NiFi, SQL Server Integration Services (SSIS), Knime, Databricks, Azure Data Factory'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache NiFi, Apache Airflow, Alteryx, SQL Server Integration Services (SSIS), Databricks, Azure Data Factory'\n",
            " 'Pentaho, Stitch' 'Scripts Python, Databricks, Azure Data Factory'\n",
            " 'Scripts Python, SQL & Stored Procedures, Oracle Data Integrator'\n",
            " 'Scripts Python, AWS Glue, Apache Airflow'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Scripts Python, Databricks'\n",
            " 'Apache Airflow, Google Dataflow, Databricks'\n",
            " 'SQL & Stored Procedures, Pentaho, Oracle Data Integrator, Databricks'\n",
            " 'Databricks, Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, AWS Glue, Apache Airflow, Google Dataflow, SQL Server Integration Services (SSIS)'\n",
            " 'SAS Data Integration, Databricks'\n",
            " 'IBM DataStage, SQL & Stored Procedures, SQL Server Integration Services (SSIS), AWS Glue'\n",
            " 'SQL & Stored Procedures, Oracle Data Integrator, SQL Server Integration Services (SSIS)'\n",
            " 'Apache Airflow, SQL & Stored Procedures, Scripts Python, SQL Server Integration Services (SSIS)']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.1_Scripts Python':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.2_SQL & Stored Procedures':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.3_Apache Airflow':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.4_Apache NiFi':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.5_Luigi':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.6_AWS Glue':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.7_Talend':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.8_Pentaho':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.9_Alteryx':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.10_Stitch':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.11_Fivetran':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.12_Google Dataflow':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.13_Oracle Data Integrator':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.14_IBM DataStage':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.15_SAP BW ETL':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.16_SQL Server Integration Services (SSIS)':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.17_SAS Data Integration':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.18_Qlik Sense':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.19_Knime':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.20_Databricks':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.21_Não utilizo ferramentas de ETL':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.c_possui_data_lake':\n",
            "[nan True False]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.d_tecnologia_data_lake':\n",
            "[nan 'Amazon S3 + Redshift + Athena' 'Azure Datalake'\n",
            " 'Google FS + BigQuery' 'Databricks' 'Snowflake' 'Hadoop + Hive' 'Presto']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.e_possui_data_warehouse':\n",
            "[nan True False]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.f_tecnologia_data_warehouse':\n",
            "[nan 'AWS Redshift' 'Azure' 'Google BigQuery' 'Databricks' 'Snowflake'\n",
            " 'Postgres/MySQL' 'Oracle' 'Presto' 'Teradata' 'IBM']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.g_ferramentas_de_qualidade_de_dados_(dia_a_dia)':\n",
            "[nan 'AWS Deequ' 'dbt' 'Atlan' 'Anomalo' 'DataHub Project'\n",
            " 'Apache Griffin' 'Metaplane' 'great\\\\_expectations' 'Open Metadata'\n",
            " 'Monte Carlo' 'Amundsen' 'Datafold' 'SODA' 'Data Band' 'Big Eye'\n",
            " 'Acceldata']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h_maior_tempo_gasto_como_de':\n",
            "[nan\n",
            " 'Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.\"\n",
            " \"Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " \"Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Cuidando da qualidade dos dados, metadados e dicionário de dados.'\n",
            " \"Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Cuidando da qualidade dos dados, metadados e dicionário de dados.\"\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.\"\n",
            " 'Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " \"Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Cuidando da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Cuidando da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Cuidando da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Cuidando da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses., Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " \"Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Cuidando da qualidade dos dados, metadados e dicionário de dados.'\n",
            " 'Cuidando da qualidade dos dados, metadados e dicionário de dados., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Cuidando da qualidade dos dados, metadados e dicionário de dados., Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Cuidando da qualidade dos dados, metadados e dicionário de dados., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc., Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.\"\n",
            " 'Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.'\n",
            " 'Cuidando da qualidade dos dados, metadados e dicionário de dados., Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc., Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " 'Cuidando da qualidade dos dados, metadados e dicionário de dados., Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " \"Cuidando da qualidade dos dados, metadados e dicionário de dados., Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.'\n",
            " 'Cuidando da qualidade dos dados, metadados e dicionário de dados., Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " \"Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"\n",
            " 'Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc., Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.'\n",
            " 'Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " \"Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação., Realizando construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.\"]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.1_Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.2_Realizando construções de ETL\\s em ferramentas como Pentaho, Talend, Dataflow etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.3_Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.4_Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.5_Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.6_Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.7_Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.8_Cuidando da qualidade dos dados, metadados e dicionário de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.9_Nenhuma das opções listadas refletem meu dia a dia.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a_rotina_como_da':\n",
            "['Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " nan 'Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Nenhuma das opções listadas refletem meu dia a dia., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " 'Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Utilizo API's para extrair dados e complementar minhas análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " 'Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " 'Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " 'Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo API's para extrair dados e complementar minhas análises., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizo API's para extrair dados e complementar minhas análises., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processo e analiso dados utilizando linguagens de programação como Python, R etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizo API's para extrair dados e complementar minhas análises., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Processo e analiso dados utilizando linguagens de programação como Python, R etc., Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvo/cuido da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.\"]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.1_Processo e analiso dados utilizando linguagens de programação como Python, R etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.2_Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.3_Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.4_Utilizo API\\s para extrair dados e complementar minhas análises.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.5_Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.6_Desenvolvo/cuido da manutenção de ETL\\s utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.7_Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.8_Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.9_Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.10_Nenhuma das opções listadas refletem meu dia a dia.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b_ferramentas_etl_da':\n",
            "['Scripts Python' nan 'Não utilizo ferramentas de ETL'\n",
            " 'Scripts Python, SQL & Stored Procedures' 'Talend'\n",
            " 'SQL & Stored Procedures, Scripts Python'\n",
            " 'Apache NiFi, Scripts Python, SQL & Stored Procedures, Apache Airflow'\n",
            " 'Databricks' 'SQL & Stored Procedures'\n",
            " 'Scripts Python, Pentaho, Fivetran'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, SAS Data Integration'\n",
            " 'SQL & Stored Procedures, Apache Airflow'\n",
            " 'SQL & Stored Procedures, Scripts Python, Databricks' 'SAP Datasphere'\n",
            " 'Scripts Python, Apache Airflow, Pentaho'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow'\n",
            " 'SQL & Stored Procedures, Scripts Python, SAS Data Integration, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Fivetran'\n",
            " 'Google Dataflow, Apache Airflow'\n",
            " 'Scripts Python, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, Power Query' 'Scripts Python, Databricks'\n",
            " 'Scripts Python, Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks'\n",
            " 'Scripts Python, AWS Glue'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho'\n",
            " 'SQL Server Integration Services (SSIS), SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, Scripts Python, Pentaho, Databricks'\n",
            " 'Dataflow power bi' 'SQL & Stored Procedures, Apache NiFi'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Databricks'\n",
            " 'Power Query' 'Apache Airflow' 'SQL Server Integration Services (SSIS)'\n",
            " 'SQL & Stored Procedures, Databricks, SQL Server Integration Services (SSIS), Scripts Python'\n",
            " 'ECL' 'Databricks, Scripts Python' 'IBM DataStage'\n",
            " 'SQL & Stored Procedures, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Knime'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS)'\n",
            " 'Databricks, Scripts Python, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Google Dataflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue' 'dbt'\n",
            " 'AWS Glue, Scripts Python, SQL Server Integration Services (SSIS), IBM DataStage'\n",
            " 'Scripts Python, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Stitch'\n",
            " 'Scripts Python, Apache Airflow, Databricks'\n",
            " 'Scripts Python, Databricks, SAP BW ETL, SAP DATASPHERE' 'Qlik Sense'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho, PlSQL Developer'\n",
            " 'Pentaho' 'Pentaho, Scripts Python' 'Dataprep' 'HEX'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, AWS Glue'\n",
            " 'SQL & Stored Procedures, Pentaho'\n",
            " 'Scripts Python, Databricks, Scripts R' 'Luigi, SAP BW ETL'\n",
            " 'Scripts Python, R' 'Databricks, Denodo'\n",
            " 'Scripts Python, Fivetran, Databricks, SQL & Stored Procedures'\n",
            " 'Pentaho, SQL & Stored Procedures' 'SAP BW ETL'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS), Qlik Sense'\n",
            " 'Não Utilizo' 'AWS Glue' 'Knime, Scripts Python, SQL & Stored Procedures'\n",
            " 'Knime, Não utilizo ferramentas de ETL' 'IBM DataStage, Apache NiFi'\n",
            " 'Databricks, Interna' 'Scripts Python, Prefect'\n",
            " 'Scripts Python, Databricks, SQL & Stored Procedures'\n",
            " 'Databricks, Apache Airflow, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Alteryx, AWS Glue'\n",
            " 'Google Dataflow'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, AWS Glue, Power Query'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS), Pentaho, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, SQL Server Integration Services (SSIS)'\n",
            " 'Qlik Sense, Apache Airflow'\n",
            " 'Apache Airflow, SQL & Stored Procedures, Qlik Sense'\n",
            " 'SQL & Stored Procedures, Scripts Python, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho, Databricks'\n",
            " 'Google Dataflow, Scripts Python, Databricks'\n",
            " 'SQL & Stored Procedures, DBT' 'Azure synapse analytics'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Databricks'\n",
            " 'Apache Airflow, Scripts Python' 'Apache Airflow, Databricks'\n",
            " 'Scripts Python, Pentaho, Databricks, AWS Glue'\n",
            " 'Scripts Python, Databricks, SAS Data Integration'\n",
            " 'Alteryx, SQL & Stored Procedures'\n",
            " 'Apache Airflow, Alteryx, SQL & Stored Procedures'\n",
            " 'Oracle Data Integrator, Google Dataflow'\n",
            " 'SQL & Stored Procedures, Scripts Python, Pentaho'\n",
            " 'Apache Airflow, Apache NiFi, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Cognos Analytcs'\n",
            " 'Não utilizo ferramentas de ETL, SQL & Stored Procedures' 'R'\n",
            " 'Pentaho, Google Dataflow' 'Excel' 'IBM Cognos Analytcs'\n",
            " 'Pentaho, Databricks' 'Scripts Python, SQL & Stored Procedures, Dagster'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Scripts Python'\n",
            " 'SQL & Stored Procedures, Power Query'\n",
            " 'Google Dataflow, SQL & Stored Procedures' 'Apache NiFi' 'HPCC'\n",
            " 'Scripts Python, Dagster' 'Apache Airflow, Databricks, Scripts Python'\n",
            " 'Google Dataflow, Scripts Python, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, SSIS' 'Knime'\n",
            " 'Pentaho, Scripts Python, SQL & Stored Procedures'\n",
            " 'Scripts Python, Databricks, Apache Airflow'\n",
            " 'Scripts Python, Qlik Sense, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, Google Dataflow' 'Microsoft Dataflow'\n",
            " 'SAS Data Integration'\n",
            " 'Scripts Python, Apache Airflow, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, SAS Data Integration'\n",
            " 'Knime, Qlik Sense' 'Power query'\n",
            " 'Scripts Python, Databricks, SQL Server Integration Services (SSIS), Apache Airflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, Pentaho, Databricks'\n",
            " 'SQL & Stored Procedures, Alteryx, Pentaho, AWS Glue, Google Dataflow, Qlik Sense, Knime, Databricks'\n",
            " 'Qlik Sense, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Qlik Sense' 'AWS Glue, Alteryx'\n",
            " 'SQL & Stored Procedures, Scripts Python, Apache Airflow, Databricks'\n",
            " 'Apache Airflow, Scripts Python, Databricks'\n",
            " 'SQL & Stored Procedures, Databricks, Scripts Python'\n",
            " 'Fivetran, Databricks' 'Scripts Python, Qlik Sense'\n",
            " 'SQL & Stored Procedures, SAS Data Integration, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Alteryx'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures' 'Scripts R'\n",
            " 'Scripts Python, Google Dataprep'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, AWS Glue'\n",
            " 'Scripts Python, Google Dataflow'\n",
            " 'Apache Airflow, Scripts Python, SQL & Stored Procedures'\n",
            " 'Scripts Python, AWS Glue, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Power Query'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Google Dataflow, Databricks'\n",
            " 'Scripts Python, Alteryx, Knime' 'Databricks, Apache Airflow'\n",
            " 'IBM DataStage, Google Dataflow, Scripts Python, SQL & Stored Procedures'\n",
            " 'Scripts Python, Knime, Databricks'\n",
            " 'Scripts Python, SAS Data Integration'\n",
            " 'Databricks, SAS Data Integration, Oracle Data Integrator'\n",
            " 'Oracle Data Integrator, Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Alteryx'\n",
            " 'Scripts Python, awk'\n",
            " 'SQL & Stored Procedures, AWS Glue, Scripts Python, dbt'\n",
            " 'Scripts Python, SQL & Stored Procedures, Alteryx, Knime'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Alteryx, SAS Data Integration'\n",
            " 'SQL & Stored Procedures, AWS Glue'\n",
            " 'Não utilizo ferramentas de ETL, Databricks'\n",
            " 'SQL & Stored Procedures, Scripts Python, Apache Airflow'\n",
            " 'API própria para extração de DW'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Alteryx, Qlik Sense'\n",
            " 'SAP BW ETL, Databricks'\n",
            " 'Apache Airflow, SQL & Stored Procedures, Scripts Python'\n",
            " 'Scripts Python, Pipeline Pilot'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Alteryx, SQL Server Integration Services (SSIS)'\n",
            " 'Apache Airflow, Google Dataform' 'Apache Airflow, hex'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho, Alteryx, Databricks'\n",
            " 'Datalink' 'Metabase'\n",
            " 'Scripts Python, SQL & Stored Procedures, Alteryx, Oracle Data Integrator'\n",
            " 'Apache Airflow, SQL & Stored Procedures, Google Dataflow, Databricks'\n",
            " 'Apache Airflow, Databricks, SQL & Stored Procedures, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS), Fabric'\n",
            " 'Scripts Python, SQL & Stored Procedures, DBT' 'Qlik Sense, SAP BW ETL'\n",
            " 'Dbt' 'Powerquery' 'Oracle Data Integrator'\n",
            " 'Apache Airflow, AWS Glue, Scripts Python'\n",
            " 'Alteryx, Scripts Python, SQL & Stored Procedures, Databricks'\n",
            " 'SQL & Stored Procedures, SAS Data Integration, Scripts Python'\n",
            " 'Databricks, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, dbt' 'Scripts Python, Pentaho, Databricks'\n",
            " 'Apache Airflow, Fivetran'\n",
            " 'SQL & Stored Procedures, Scripts Python, Google Dataflow'\n",
            " 'Scripts Python, SQL & Stored Procedures, Azure Synapse'\n",
            " 'Scripts Python, Pentaho, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, Alteryx' 'dbt Cloud'\n",
            " 'Scripts Python, SQL & Stored Procedures, Fivetran, Databricks'\n",
            " 'SAS Data Integration, SQL Server Integration Services (SSIS)'\n",
            " 'Fork próprio de airflow'\n",
            " 'SQL & Stored Procedures, Databricks, SAS Enterprise Guide' 'Power bi'\n",
            " 'Apache Airflow, Scripts Python, SQL & Stored Procedures, Databricks'\n",
            " 'Scripts Python, AWS Glue, SQL & Stored Procedures' 'Alteryx'\n",
            " 'SQL & Stored Procedures, Talend' 'Scripts Python, Power Automate'\n",
            " 'Fivetran, Dataform' 'SAS Data Integration, Databricks'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Scripts Python, Fivetran, Databricks'\n",
            " 'Scripts Python, Power BI'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Google Dataflow'\n",
            " 'Dremio' 'Scripts Python, Não utilizo ferramentas de ETL'\n",
            " 'Scripts Python, Apache Airflow, SQL & Stored Procedures, Talend'\n",
            " 'Scripts Python, Pentaho'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'SQL & Stored Procedures, Pentaho, SQL Server Integration Services (SSIS)'\n",
            " 'SQL & Stored Procedures, SAP BW ETL'\n",
            " 'Fivetran, Scripts Python, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, SAS Data Integration'\n",
            " 'Qlik Sense, SQL & Stored Procedures, Scripts Python'\n",
            " 'Scripts Python, AWS Glue, Alteryx' 'Power BI' 'Scripts Python, PowerBI'\n",
            " 'SAS Data Integration, Google Dataflow'\n",
            " 'Databricks, SQL & Stored Procedures' 'Faço dentro do PowerQuery'\n",
            " 'Scripts Python, SQL & Stored Procedures, SQL Server Integration Services (SSIS), Knime'\n",
            " 'Apache Airflow, SQL & Stored Procedures'\n",
            " 'Scripts Python, SQL & Stored Procedures, Alteryx, Google Dataflow'\n",
            " 'Somente o Power Query, porque está integrado ao Excel e ao Power BI.'\n",
            " 'SQL & Stored Procedures, Pentaho, SAS Data Integration'\n",
            " 'Scripts Python, SQL & Stored Procedures, Oracle Data Integrator, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, SQL & Stored Procedures, Pentaho, SQL Server Integration Services (SSIS), Qlik Sense'\n",
            " 'SQL & Stored Procedures, Scripts Python, Databricks, SAS Data Integration'\n",
            " 'SQL & Stored Procedures, Scripts Python, SQL Server Integration Services (SSIS), Apache Hop'\n",
            " 'SQL & Stored Procedures, Qlik Sense'\n",
            " 'Databricks, Pentaho, Knime, Qlik Sense' 'Pentaho, AWS Glue'\n",
            " 'Qlik Sense ou Qlikview' 'Ferramenta interna da empresa'\n",
            " 'Scripts Python, Dataflows Gen2 Microsoft Fabric'\n",
            " 'SQL & Stored Procedures, Camada semântica do Looker Platform'\n",
            " 'Scripts Python, Qlik Sense, Databricks'\n",
            " 'Knime, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, Apache Airflow, Apache NiFi'\n",
            " 'AWS Glue, SAS Data Integration'\n",
            " 'Scripts Python, SQL & Stored Procedures, Dataprep(Google Trifacta)'\n",
            " 'Oracle Data Integrator, Scripts Python, Databricks'\n",
            " 'Scripts Python, Dataprep'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Talend, Pentaho, Alteryx, Databricks'\n",
            " 'SQL & Stored Procedures, Google App Script'\n",
            " 'Databricks, Não utilizo ferramentas de ETL'\n",
            " 'SQL & Stored Procedures, Talend, Pentaho'\n",
            " 'Scripts Python, Apache Airflow, Google Dataflow, Qlik Sense, Databricks, SQL Server Integration Services (SSIS)'\n",
            " 'Scripts Python, Alteryx, Pentaho' 'Databricks, Power BI' 'ACL'\n",
            " 'AWS Glue, Scripts Python, SQL & Stored Procedures'\n",
            " 'SQL & Stored Procedures, Oracle Data Integrator'\n",
            " 'Databricks, Power Query'\n",
            " 'Não utilizo ferramentas de ETL, Scripts Python' 'Stitch'\n",
            " 'Talend, Apache NiFi'\n",
            " 'Scripts Python, SQL & Stored Procedures, AWS Glue, Oracle Data Integrator, SQL Server Integration Services (SSIS), Qlik Sense, Knime'\n",
            " 'Scripts Python, SQL & Stored Procedures, Oracle Data Integrator, Databricks'\n",
            " 'Oracle Data Integrator, IBM DataStage' 'Alteryx, Dataprep'\n",
            " 'Databricks, SQL & Stored Procedures, Scripts Python'\n",
            " 'Talend, IBM DataStage, Qlik Sense, Scripts Python, SQL & Stored Procedures'\n",
            " 'IBM DataStage, SAS Data Integration'\n",
            " 'Qlik Sense, Oracle Data Integrator'\n",
            " 'SQL & Stored Procedures, AWS Glue, Scripts Python'\n",
            " 'SQL Server Integration Services (SSIS), Scripts Python, Apache Airflow'\n",
            " 'SQL Server Integration Services (SSIS), SAS Data Integration'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, AWS Glue, Stitch'\n",
            " 'SQL & Stored Procedures, AWS Glue, SAS Data Integration'\n",
            " 'Scripts Python, Pentaho, Knime'\n",
            " 'Scripts Python, SQL & Stored Procedures, Knime, Databricks'\n",
            " 'Power BI Service' 'Alteryx, AWS Glue, Scripts Python' 'Interna'\n",
            " 'SQL & Stored Procedures, Knime' 'Scripts Python, Knime'\n",
            " 'Scripts Python, Airbyte'\n",
            " 'Scripts Python, SQL & Stored Procedures, IBM DataStage'\n",
            " 'Databricks, Scripts Python, Alteryx'\n",
            " 'Alteryx, Pentaho, Google Cloud Dataprep by Trifacta'\n",
            " 'Qlik Sense, MS Power Query'\n",
            " 'SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, Qlik Sense, Databricks'\n",
            " 'Scripts em R' 'Não utilizo ferramentas de ETL, Power Query'\n",
            " 'Pentaho, Proprio da Empresa'\n",
            " 'Pentaho, SQL & Stored Procedures, Scripts Python'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Pentaho'\n",
            " 'SQL & Stored Procedures, Scripts Python, Google Dataprep'\n",
            " 'SQL & Stored Procedures, SQL Server Integration Services (SSIS), Databricks, ADF'\n",
            " 'Scripts Python, Apache Airflow, dbt'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache NiFi'\n",
            " 'Scripts Python, SQL & Stored Procedures, Apache Airflow, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, Qlik Sense, Apache Airflow'\n",
            " 'AWS Glue, SQL & Stored Procedures'\n",
            " 'Scripts Python, Apache Airflow, Pentaho, Qlik Sense, Knime'\n",
            " 'Scripts Python, Databricks, Pentaho'\n",
            " 'Scripts Python, SQL & Stored Procedures, Databricks, AWS Glue, TERADATA'\n",
            " 'Power Query, linguagem M'\n",
            " 'SQL & Stored Procedures, Scripts Python, Alteryx'\n",
            " 'SQL & Stored Procedures, Apache Airflow, Qlik Sense'\n",
            " 'Databricks, AWS Glue'\n",
            " 'SQL Server Integration Services (SSIS), SQL & Stored Procedures, MS Fabric'\n",
            " 'SQL & Stored Procedures, Scripts Python, Oracle Data Integrator, Databricks'\n",
            " 'Oracle Data Integrator, Knime' 'EXCEL'\n",
            " 'Scripts Python, AWS Glue, IBM DataStage, SQL Server Integration Services (SSIS)'\n",
            " 'SQL & Stored Procedures, Pentaho, Scripts Python'\n",
            " 'Scripts Python, SQL & Stored Procedures, Azure Data Factory'\n",
            " 'Microsoft Power BI'\n",
            " 'Alteryx, SQL & Stored Procedures, SQL Server Integration Services (SSIS), Databricks'\n",
            " 'Scripts Python, SQL & Stored Procedures, SAS Data Integration, Databricks'\n",
            " 'Qlik Sense, PowerCenter']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.1_Scripts Python':\n",
            "[ 1. nan  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.2_SQL & Stored Procedures':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.3_Apache Airflow':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.4_Apache NiFi':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.5_Luigi':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.6_AWS Glue':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.7_Talend':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.8_Pentaho':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.9_Alteryx':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.10_Stitch':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.11_Fivetran':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.12_Google Dataflow':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.13_Oracle Data Integrator':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.14_IBM DataStage':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.15_SAP BW ETL':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.16_SQL Server Integration Services (SSIS)':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.17_SAS Data Integration':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.18_Qlik Sense':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.19_Knime':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.20_Databricks':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.21_Não utilizo ferramentas de ETL':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c_ferramentas_autonomia_area_de_negocios':\n",
            "['Minha empresa não utiliza essas ferramentas.' 'Não sei informar.' nan\n",
            " 'Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.'\n",
            " 'Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc., Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.'\n",
            " 'Minha empresa não utiliza essas ferramentas., Não sei informar.'\n",
            " 'Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc., \"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc.'\n",
            " 'Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.'\n",
            " '\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc.'\n",
            " 'Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics., Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.'\n",
            " '\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc., Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.'\n",
            " 'Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards., Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.'\n",
            " 'Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics.'\n",
            " 'Não sei informar., Minha empresa não utiliza essas ferramentas.'\n",
            " 'Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards., Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics.'\n",
            " 'Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards., Não sei informar.'\n",
            " 'Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics., Não sei informar.'\n",
            " '\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc., Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics.'\n",
            " 'Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics., Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc., Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.'\n",
            " 'Não sei informar., Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.'\n",
            " '\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc., Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards., Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.'\n",
            " '\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc., Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards., Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics.'\n",
            " 'Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics., \"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc.'\n",
            " 'Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics., Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.'\n",
            " 'Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards., \"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc.'\n",
            " '\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc., Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics., Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.'\n",
            " 'Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards., Minha empresa não utiliza essas ferramentas.'\n",
            " '\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc., Não sei informar.'\n",
            " 'Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards., Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics., \"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc., Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.1_Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.2_\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.3_Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.4_Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.5_Minha empresa não utiliza essas ferramentas.':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.6_Não sei informar.':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d_maior_tempo_gasto_como_da':\n",
            "['Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " nan 'Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " 'Processando e analisando dados utilizando linguagens de programação como Python, R etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Processando e analisando dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Processando e analisando dados utilizando linguagens de programação como Python, R etc., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc.'\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Processando e analisando dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Processando e analisando dados utilizando linguagens de programação como Python, R etc., Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Processando e analisando dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc., Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.'\n",
            " \"Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Processando e analisando dados utilizando linguagens de programação como Python, R etc., Utilizando API's para extrair dados e complementar minhas análises.\"\n",
            " \"Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Utilizando API's para extrair dados e complementar minhas análises.\"\n",
            " \"Processando e analisando dados utilizando linguagens de programação como Python, R etc., Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " 'Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio., Processando e analisando dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.'\n",
            " \"Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc., Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Processando e analisando dados utilizando linguagens de programação como Python, R etc., Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " 'Processando e analisando dados utilizando linguagens de programação como Python, R etc., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc.'\n",
            " \"Utilizando API's para extrair dados e complementar minhas análises.\"\n",
            " \"Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc.'\n",
            " \"Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Processando e analisando dados utilizando linguagens de programação como Python, R etc.\"\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc.'\n",
            " \"Utilizando API's para extrair dados e complementar minhas análises., Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.\"\n",
            " \"Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc.\"\n",
            " 'Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " \"Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Utilizando API's para extrair dados e complementar minhas análises.\"\n",
            " 'Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Processando e analisando dados utilizando linguagens de programação como Python, R etc.'\n",
            " 'Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc., Processando e analisando dados utilizando linguagens de programação como Python, R etc.'\n",
            " \"Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio., Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Utilizando API's para extrair dados e complementar minhas análises., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.\"\n",
            " 'Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " 'Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc.'\n",
            " 'Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc., Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datastes etc.'\n",
            " 'Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados., Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.'\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc., Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " 'Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.'\n",
            " \"Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio., Utilizando API's para extrair dados e complementar minhas análises.\"\n",
            " 'Processando e analisando dados utilizando linguagens de programação como Python, R etc., Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " \"Utilizando API's para extrair dados e complementar minhas análises., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.\"\n",
            " 'Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.'\n",
            " 'Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " 'Nenhuma das opções listadas refletem meu dia a dia., Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.'\n",
            " \"Utilizando API's para extrair dados e complementar minhas análises., Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.\"\n",
            " \"Utilizando API's para extrair dados e complementar minhas análises., Processando e analisando dados utilizando linguagens de programação como Python, R etc.\"]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.1_Processando e analisando dados utilizando linguagens de programação como Python, R etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.2_Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.3_Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.4_Utilizando API's para extrair dados e complementar minhas análises.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.5_Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.6_Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.7_Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.8_Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.9_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.10_Nenhuma das opções listadas refletem meu dia a dia.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a_rotina_como_ds':\n",
            "[nan\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " \"Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e gerencio soluções de Feature Store e cultura de MLOps., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " \"Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).\"\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e gerencio soluções de Feature Store e cultura de MLOps., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Crio e gerencio soluções de Feature Store e cultura de MLOps., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e gerencio soluções de Feature Store e cultura de MLOps., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e gerencio soluções de Feature Store e cultura de MLOps.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Crio e gerencio soluções de Feature Store e cultura de MLOps., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " \"Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.\"\n",
            " \"Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Crio e gerencio soluções de Feature Store e cultura de MLOps., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " \"Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.'\n",
            " 'Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento., Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados., Crio e gerencio soluções de Feature Store e cultura de MLOps., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem., Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio., Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.1_Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.2_Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.3_Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.4_Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.5_Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.6_Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.7_Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.8_Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.9_Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.10_Crio e gerencio soluções de Feature Store e cultura de MLOps.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.11_Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.12_Treino e aplico LLM's para solucionar problemas de negócio.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b_tecnicas_e_metodos_ds':\n",
            "[nan 'Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos de Visão Computacional., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de Reinforcement Learning (aprendizado por reforço).'\n",
            " \"Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " \"Desenvolvo sistemas de recomendação (RecSys)., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos de Visão Computacional., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys).\"\n",
            " 'Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " 'Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio.\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Desenvolvo sistemas de recomendação (RecSys).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo métodos estatísticos Bayesianos para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional., Desenvolvo sistemas de recomendação (RecSys).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos de Visão Computacional., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos de Visão Computacional., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos de Visão Computacional., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " 'Utilizo métodos estatísticos Bayesianos para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de Detecção de Churn., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos de Visão Computacional., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " \"Utilizo modelos de Machine Learning para detecção de fraude., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo cadeias de Markov ou HMM's para realizar análises de dados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Desenvolvo sistemas de recomendação (RecSys).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos de Visão Computacional.'\n",
            " 'Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de Detecção de Churn., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys).'\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo modelos de Detecção de Churn., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " 'Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo LLM's para solucionar problemas de negócio.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos estatísticos Bayesianos para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo LLM's para solucionar problemas de negócio.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos de Visão Computacional., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos estatísticos Bayesianos para analisar dados.'\n",
            " \"Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Desenvolvo sistemas de recomendação (RecSys)., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos estatísticos Bayesianos para analisar dados.'\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo LLM's para solucionar problemas de negócio.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Desenvolvo sistemas de recomendação (RecSys).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.'\n",
            " 'Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " 'Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de Detecção de Churn., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Reinforcement Learning (aprendizado por reforço).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de Detecção de Churn., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " \"Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " 'Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.\"\n",
            " 'Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Detecção de Churn., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos Bayesianos para analisar dados.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo métodos de Visão Computacional., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo métodos de Visão Computacional.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"\n",
            " 'Utilizo modelos de Machine Learning para detecção de fraude., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo LLM's para solucionar problemas de negócio.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de regressão (linear, logística, GLM).'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo modelos de Reinforcement Learning (aprendizado por reforço)., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " \"Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de regressão (linear, logística, GLM).\"\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Desenvolvo sistemas de recomendação (RecSys)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " 'Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.'\n",
            " 'Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Realizo previsões através de modelos de Séries Temporais (Time Series).'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo modelos de Detecção de Churn.'\n",
            " \"Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional.\"\n",
            " \"Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos de Visão Computacional.\"\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo modelos de Detecção de Churn.'\n",
            " 'Utilizo modelos de regressão (linear, logística, GLM)., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude.'\n",
            " 'Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo modelos de Machine Learning para detecção de fraude.\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series).\"\n",
            " 'Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Utilizo métodos de Visão Computacional., Utilizo métodos estatísticos Bayesianos para analisar dados.'\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc).\"\n",
            " \"Utilizo modelos de regressão (linear, logística, GLM)., Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação., Utilizo LLM's para solucionar problemas de negócio., Utilizo métodos estatísticos Bayesianos para analisar dados., Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados., Utilizo cadeias de Markov ou HMM's para realizar análises de dados., Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatística) para analisar dados., Utilizo técnicas de Clusterização (K-means, Spectral, DBScan etc)., Realizo previsões através de modelos de Séries Temporais (Time Series)., Utilizo modelos de Machine Learning para detecção de fraude., Utilizo métodos de Visão Computacional., Utilizo modelos de Detecção de Churn.\"]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.1_Utilizo modelos de regressão (linear, logística, GLM).':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.2_Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.3_Desenvolvo sistemas de recomendação (RecSys).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.4_Utilizo métodos estatísticos Bayesianos para analisar dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.5_Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.6_Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatistica) para analisar dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.7_Utilizo cadeias de Markov ou HMM\\s para realizar análises de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.8_Desenvolvo técnicas de Clusterização (K-means, Spectral, DBScan etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.9_Realizo previsões através de modelos de Séries Temporais (Time Series).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.10_Utilizo modelos de Reinforcement Learning (aprendizado por reforço).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.11_Utilizo modelos de Machine Learning para detecção de fraude.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.12_Utilizo métodos de Visão Computacional.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.13_Utilizo modelos de Detecção de Churn.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.14_Utilizo LLM's para solucionar problemas de negócio.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c_tecnologias_ds':\n",
            "[nan\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Não utilizo nenhuma dessas ferramentas no meu dia a dia.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Planilhas (Excel, Google Sheets etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Não utilizo nenhuma dessas ferramentas no meu dia a dia., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Planilhas (Excel, Google Sheets etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Planilhas (Excel, Google Sheets etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).'\n",
            " 'Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Ferramentas de estatística avançada como SPSS, SAS etc.'\n",
            " 'Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc)., Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Planilhas (Excel, Google Sheets etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).'\n",
            " 'Planilhas (Excel, Google Sheets etc)., Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda)., Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc)., Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc)., Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.1_Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.2_Planilhas (Excel, Google Sheets etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.3_Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.4_Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.5_Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.6_Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.7_Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.8_Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.9_Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.10_Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.11_Ferramentas de estatística avançada como SPSS, SAS, etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d_maior_tempo_gasto_como_ds':\n",
            "[nan\n",
            " \"Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.\"\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem., Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem.'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " \"Coletando e limpando dos dados que uso para análise e modelagem., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem., Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Coletando e limpando dos dados que uso para análise e modelagem.'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.\"\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Coletando e limpando dos dados que uso para análise e modelagem.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " \"Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Coletando e limpando dos dados que uso para análise e modelagem.'\n",
            " 'Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem., Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Coletando e limpando dos dados que uso para análise e modelagem.\"\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Nenhuma das opções listadas refletem meu dia a dia.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).\"\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.\"\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem., Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.\"\n",
            " 'Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados., Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Criando e gerenciando soluções de Feature Store e cultura de MLOps., Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.), Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento., Coletando e limpando dos dados que uso para análise e modelagem.'\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.'\n",
            " 'Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados)., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento., Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " 'Criando e gerenciando soluções de Feature Store e cultura de MLOps., Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados., Coletando e limpando dos dados que uso para análise e modelagem.'\n",
            " 'Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.'\n",
            " 'Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Criando e gerenciando soluções de Feature Store e cultura de MLOps.\"\n",
            " \"Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento., Treinando e aplicando LLM's para solucionar problemas de negócio.\"\n",
            " 'Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento., Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.\"\n",
            " 'Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.'\n",
            " 'Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados., Criando e gerenciando soluções de Feature Store e cultura de MLOps.'\n",
            " 'Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento., Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.'\n",
            " 'Criando e gerenciando soluções de Feature Store e cultura de MLOps.'\n",
            " 'Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)'\n",
            " 'Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados., Criando e gerenciando soluções de Feature Store e cultura de MLOps.'\n",
            " 'Criando e gerenciando soluções de Feature Store e cultura de MLOps., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " \"Treinando e aplicando LLM's para solucionar problemas de negócio., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.\"\n",
            " 'Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc., Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.'\n",
            " 'Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados., Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).'\n",
            " 'Coletando e limpando dos dados que uso para análise e modelagem., Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.'\n",
            " 'Nenhuma das opções listadas refletem meu dia a dia., Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.1_Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.2_Coletando e limpando dos dados que uso para análise e modelagem.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.3_Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.4_Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.5_Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.6_Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.7_Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.8_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.9_Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.10_Criando e gerenciando soluções de Feature Store e cultura de MLOps.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.11_Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Contagem de valores únicos por coluna:\n",
            "0.a_token                                                                                                            5215\n",
            "0.d_data/hora_envio                                                                                                  5188\n",
            "1.a_idade                                                                                                              49\n",
            "1.a.1_faixa_idade                                                                                                       9\n",
            "1.b_genero                                                                                                              4\n",
            "                                                                                                                     ... \n",
            "8.d.8_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.                2\n",
            "8.d.9_Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.                                      2\n",
            "8.d.10_Criando e gerenciando soluções de Feature Store e cultura de MLOps.                                              2\n",
            "8.d.11_Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)       2\n",
            "8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.                                                2\n",
            "Length: 403, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir apenas colunas com menos de 10 valores únicos\n",
        "for column in df.columns:\n",
        "    if df[column].nunique() <= 10:\n",
        "        print(f\"Valores únicos na coluna '{column}':\")\n",
        "        print(df[column].unique())\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S5iDavhy3YcP",
        "outputId": "d82f8b5d-112b-458d-db5e-5c5daa155dd7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores únicos na coluna '1.a.1_faixa_idade':\n",
            "['17-21' '22-24' '25-29' '30-34' '35-39' '40-44' '45-49' '50-54' '55+']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.b_genero':\n",
            "['Masculino' 'Feminino' 'Outro' 'Prefiro não informar']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.c_cor/raca/etnia':\n",
            "['Branca' 'Parda' 'Preta' 'Outra' 'Amarela' 'Prefiro não informar'\n",
            " 'Indígena']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.d_pcd':\n",
            "['Não' 'Sim' 'Prefiro não informar']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.e.1_Não acredito que minha experiência profissional seja afetada':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.e.2_Sim, devido a minha Cor/Raça/Etnia':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.e.3_Sim, devido a minha identidade de gênero':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.e.4_Sim, devido ao fato de ser PCD':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.1_Quantidade de oportunidades de emprego/vagas recebidas':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.2_Senioridade das vagas recebidas em relação à sua experiência':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.3_Aprovação em processos seletivos/entrevistas':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.4_Oportunidades de progressão de carreira':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.5_Velocidade de progressão de carreira':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.6_Nível de cobrança no trabalho/Stress no trabalho':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.7_Atenção dada pelas pessoas diante das minhas opiniões e ideias':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.8_Relação com outras pessoas da empresa, em momentos de trabalho':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.f.9_Relação com outras pessoas da empresa, em momentos de integração e outros momentos fora do trabalho':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.i.2_regiao_onde_mora':\n",
            "['Sul' 'Sudeste' 'Centro-oeste' 'Nordeste' 'Norte' nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.k.2_regiao_de_origem':\n",
            "[nan 'Sul' 'Sudeste' 'Centro-oeste' 'Nordeste' 'Norte']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.g_vive_no_brasil':\n",
            "[ True False]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.j_vive_no_estado_de_formacao':\n",
            "[True False nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.l_nivel_de_ensino':\n",
            "['Estudante de Graduação' 'Graduação/Bacharelado'\n",
            " 'Não tenho graduação formal' 'Prefiro não informar' 'Pós-graduação'\n",
            " 'Doutorado ou Phd' 'Mestrado']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '1.m_área_de_formação':\n",
            "['Computação / Engenharia de Software / Sistemas de Informação/ TI'\n",
            " 'Economia/ Administração / Contabilidade / Finanças/ Negócios' nan\n",
            " 'Estatística/ Matemática / Matemática Computacional/ Ciências Atuariais'\n",
            " 'Outra opção'\n",
            " 'Outras Engenharias (não incluir engenharia de software ou TI)'\n",
            " 'Ciências Biológicas/ Farmácia/ Medicina/ Área da Saúde'\n",
            " 'Marketing / Publicidade / Comunicação / Jornalismo / Ciências Sociais'\n",
            " 'Química / Física']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.c_numero_de_funcionarios':\n",
            "['de 101 a 500' 'Acima de 3.000' 'de 501 a 1.000' 'de 1.001 a 3.000' nan\n",
            " 'de 6 a 10' 'de 11 a 50' 'de 1 a 5' 'de 51 a 100']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.d_atua_como_gestor':\n",
            "[False nan True]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.e_cargo_como_gestor':\n",
            "[nan 'Gerente/Head' 'Supervisor/Coordenador'\n",
            " 'Sócio ou C-level (CEO, CDO, CIO, CTO etc)' 'Team Leader/Tech Leader'\n",
            " 'Diretor/VP']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.g_nivel':\n",
            "['Júnior' nan 'Pleno' 'Sênior']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.i_tempo_de_experiencia_em_dados':\n",
            "['de 1 a 2 anos' 'Menos de 1 ano' 'Não tenho experiência na área de dados'\n",
            " nan 'de 5 a 6 anos' 'de 3 a 4 anos' 'de 7 a 10 anos' 'Mais de 10 anos']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.j_tempo_de_experiencia_em_ti':\n",
            "['de 1 a 2 anos' 'Menos de 1 ano'\n",
            " 'Não tive experiência na área de TI/Engenharia de Software antes de começar a trabalhar na área de dados'\n",
            " nan 'de 3 a 4 anos' 'de 5 a 6 anos' 'de 7 a 10 anos' 'Mais de 10 anos']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.k_satisfeito_atualmente':\n",
            "[True False nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.1_Remuneração/Salário':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.2_Benefícios':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.3_Propósito do trabalho e da empresa':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.4_Flexibilidade de trabalho remoto':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.5_Ambiente e clima de trabalho':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.6_Oportunidade de aprendizado e trabalhar com referências':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.7_Oportunidades de crescimento':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.8_Maturidade da empresa em termos de tecnologia e dados':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.9_Relação com os gestores e líderes':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.10_Reputação que a empresa tem no mercado':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.l.11_Gostaria de trabalhar em outra área':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.m_participou_de_entrevistas_ultimos_6m':\n",
            "['Sim, fui aprovado e mudei de emprego'\n",
            " 'Não participei de entrevistas de emprego/processos seletivos nos últimos 6 meses'\n",
            " 'Sim, fui aprovado mas decidi não mudar de emprego' nan\n",
            " 'Sim, fiz entrevistas mas não fui aprovado (ou ainda aguardo resposta)'\n",
            " 'Sim, fui aprovado no meu primeiro emprego (ou estava sem emprego)']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.n_planos_de_mudar_de_emprego_6m':\n",
            "['Não estou buscando, mas me considero aberto a outras oportunidades'\n",
            " 'Não estou buscando e não pretendo mudar de emprego nos próximos 6 meses'\n",
            " 'Estou em busca de oportunidades dentro ou fora do Brasil' nan\n",
            " 'Estou em busca de oportunidades, mas apenas fora do Brasil']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.1_Remuneração/Salário':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.2_Benefícios':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.3_Propósito do trabalho e da empresa':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.4_Flexibilidade de trabalho remoto':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.5_Ambiente e clima de trabalho':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.6_Oportunidade de aprendizado e trabalhar com referências':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.7_Plano de carreira e oportunidades de crescimento':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.8_Maturidade da empresa em termos de tecnologia e dados':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.9_Qualidade dos gestores e líderes':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.o.10_Reputação que a empresa tem no mercado':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.q_empresa_passou_por_layoff_em_2024':\n",
            "['Sim, ocorreram layoffs/demissões em massa na empresa em que trabalho mas não fui afetado'\n",
            " 'Não ocorreram layoffs/demissões em massa na empresa em que trabalho' nan\n",
            " 'Sim, ocorreram layoffs/demissões em massa na empresa em que trabalhava e eu fui afetado']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.r_modelo_de_trabalho_atual':\n",
            "['Modelo 100% remoto' 'Modelo 100% presencial'\n",
            " 'Modelo híbrido flexível (o funcionário tem liberdade para escolher quando estar no escritório presencialmente)'\n",
            " nan 'Modelo híbrido com dias fixos de trabalho presencial']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.s_modelo_de_trabalho_ideal':\n",
            "['Modelo híbrido flexível (o funcionário tem liberdade para escolher quando estar no escritório presencialmente)'\n",
            " 'Modelo 100% presencial'\n",
            " 'Modelo híbrido com dias fixos de trabalho presencial' nan\n",
            " 'Modelo 100% remoto']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '2.t_atitude_em_caso_de_retorno_presencial':\n",
            "['Vou procurar outra oportunidade no modelo híbrido ou remoto'\n",
            " 'Vou aceitar e retornar ao modelo 100% presencial' nan\n",
            " 'Vou procurar outra oportunidade no modelo 100% remoto']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.a_numero_de_pessoas_em_dados':\n",
            "[nan 'Ainda não temos pessoas atuando com dados na empresa' '1 - 3'\n",
            " '4 - 10' '11 - 20' '101 - 300' 'Acima de 300 pessoas' '51 - 100'\n",
            " '21 - 50']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.1_Analytics Engineer':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.2_Engenharia de Dados/Data Engineer':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.3_Analista de Dados/Data Analyst':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.4_Cientista de Dados/Data Scientist':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.5_Database Administrator/DBA':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.6_Analista de Business Intelligence/BI':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.7_Arquiteto de Dados/Data Architect':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.8_Data Product Manager/DPM':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.9_Business Analyst':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.b.10_ML Engineer/AI Engineer':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.1_Pensar na visão de longo prazo de dados':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.2_Organização de treinamentos e iniciativas':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.3_Atração, seleção e contratação':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.4_Decisão sobre contratação de ferramentas':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.5_gestor da equipe de engenharia de dados':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.6_gestor da equipe de estudos, relatórios':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.7_gestor da equipe de Inteligência Artificial e Machine Learning':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.8_Apesar de ser gestor ainda atuo na parte técnica':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.9_Gestão de projetos de dados':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.10_Gestão de produtos de dados':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.c.11_Gestão de pessoas':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.1_Contratar talentos':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.2_Reter talentos':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.3_Convencer a empresa a aumentar investimentos':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.4_Gestão de equipes no ambiente remoto':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.5_Gestão de projetos envolvendo áreas multidisciplinares':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.6_Organizar as informações com qualidade e confiabilidade':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.7_Processar e armazenar um alto volume de dados':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.8_Gerar valor para as áreas de negócios':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.9_Desenvolver e manter modelos Machine Learning em produção':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.10_Gerenciar a expectativa das áreas':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.11_Garantir a manutenção dos projetos e modelos em produção':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.12_Conseguir levar inovação para a empresa':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.13_Garantir (ROI) em projetos de dados':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.d.14_Dividir o tempo entre entregas técnicas e gestão':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.e_ai_generativa_e_llm_é_uma_prioridade?':\n",
            "[nan\n",
            " 'Mais ou menos... É uma das várias iniciativas que estamos impulsionando, mas não é uma prioridade (tratam-se de iniciativas isoladas e com pouco foco).'\n",
            " 'Sim, está entre nossas principais prioridades para os próximos 2-4 anos (com discussões de iniciativas e orçamentos de curto a médio prazo).'\n",
            " 'Não é uma iniciativa que estamos focando e não tem sido uma prioridade.'\n",
            " 'Não sei opinar sobre esse assunto.'\n",
            " 'Sim, é nossa principal prioridade como empresa (com foco executivo significativo e alocação de orçamento relevante).']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.f.1 Colaboradores usando AI generativa de forma independente e descentralizada':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.f.2 Direcionamento centralizado do uso de AI generativa':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.f.3 Desenvolvedores utilizando Copilots':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.f.4 AI Generativa e LLMs para melhorar produtos externos para os clientes finais':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.f.5 AI Generativa e LLMs para melhorar produtos internos para os colaboradores':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.f.6 IA Generativa e LLMs como principal frente do negócio':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.f.7 IA Generativa e LLMs não é prioridade':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.f.8 Não sei opinar sobre o uso de IA Generativa e LLMs na empresa':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.1 Falta de compreensão dos casos de uso':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.2 Falta de confiabilidade das saídas (alucinação dos modelos)':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.3 Incerteza em relação a regulamentação':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.4 Preocupações com segurança e privacidade de dados':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.5 Retorno sobre investimento (ROI) não comprovado de IA Generativa':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.6 Dados da empresa não estão prontos para uso de IA Generativa':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.7 Falta de expertise ou falta de recursos':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.8 Alta direção da empresa não vê valor ou não vê como prioridade':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '3.g.9 Preocupações com propriedade intelectual':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.a_funcao_de_atuacao':\n",
            "['*Análise de Dados/BI:* Extrai e cruza dados unindo diferentes fontes da informação; analisa dados visando identificar padrões, gerar insights e levantar perguntas; desenvolve dashboards, relatórios e visualizações de dados em ferramentas de BI.'\n",
            " 'Atuo na área de dados, mas não atuo em nenhuma das frentes citadas.' nan\n",
            " '*Engenharia de Dados:* Modela soluções de arquitetura de dados; define modelagens de repositórios de dados (Data Lake, Data Warehouse, Data Lakehouse); desenvolve estratégias de aquisição de dados, recuperação de informação e pipelines de dados.'\n",
            " 'Não atuo na área de dados.'\n",
            " '*Ciência de Dados/Machine Learning/AI: *Desenha e executa experimentos com o objetivo de responder perguntas do negócio; desenvolve modelos preditivos e algoritmos de Machine Learning com o objetivo de otimizar e automatizar a tomada de decisão.']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.a.1_atuacao_em_dados':\n",
            "['Análise de Dados' 'Outra atuação'\n",
            " 'Buscando oportunidade na área de dados' 'Engenharia de Dados'\n",
            " 'Ciência de Dados' 'Gestor']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.b.1_Dados relacionais (estruturados em bancos SQL)':\n",
            "[ 1. nan  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.b.2_Dados armazenados em bancos NoSQL':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.b.3_Imagens':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.b.4_Textos/Documentos':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.b.5_Vídeos':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.b.6_Áudios':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.b.7_Planilhas':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.b.8_Dados georeferenciados':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.1_Dados relacionais (estruturados em bancos SQL)':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.2_Dados armazenados em bancos NoSQL':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.3_Imagens':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.4_Textos/Documentos':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.5_Vídeos':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.6_Áudios':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.7_Planilhas':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.c.8_Dados georeferenciados':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.1_SQL':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.2_R':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.3_Python':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.4_C/C++/C#':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.5_.NET':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.6_Java':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.7_Julia':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.8_SAS/Stata':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.9_Visual Basic/VBA':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.10_Scala':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.11_Matlab':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.12_Rust':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.13_PHP':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.14_JavaScript':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.d.15_Não utilizo nenhuma das linguagens listadas':\n",
            "[ 0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.1_MySQL':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.2_Oracle':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.3_SQL SERVER':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.4_Amazon Aurora ou RDS':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.5_DynamoDB':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.6_CoachDB':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.7_Cassandra':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.8_MongoDB':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.9_MariaDB':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.10_Datomic':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.11_S3':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.12_PostgreSQL':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.13_ElasticSearch':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.14_DB2':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.15_Microsoft Access':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.16_SQLite':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.17_Sybase':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.18_Firebase':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.19_Vertica':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.20_Redis':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.21_Neo4J':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.22_Google BigQuery':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.23_Google Firestore':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.24_Amazon Redshift':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.25_Amazon Athena':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.26_Snowflake':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.27_Databricks':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.28_HBase':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.29_Presto':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.30_Splunk':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.31_SAP HANA':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.32_Hive':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.g.33_Firebird':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.1_Amazon Web Services (AWS)':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.2_Google Cloud (GCP)':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.3_Azure (Microsoft)':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.4_Oracle Cloud':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.5_IBM':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.6_Servidores On Premise/Não utilizamos Cloud':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.h.7_Cloud Própria':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.1_Microsoft PowerBI':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.2_Qlik View/Qlik Sense':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.3_Tableau':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.4_Metabase':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.5_Superset':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.6_Redash':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.7_Looker':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.8_Looker Studio(Google Data Studio)':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.9_Amazon Quicksight':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.10_Alteryx':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.11_SAP Business Objects/SAP Analytics':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.12_Oracle Business Intelligence':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.13_Salesforce/Einstein Analytics':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.14_SAS Visual Analytics':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.15_Grafana':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.16_Pentaho':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.17_Fazemos todas as análises utilizando apenas Excel ou planilhas do google':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.j.18_Não utilizo nenhuma ferramenta de BI no trabalho':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.1 Colaboradores usando AI generativa de forma independente e descentralizada':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.2 Direcionamento centralizado do uso de AI generativa':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.3 Desenvolvedores utilizando Copilots':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.4 AI Generativa e LLMs para melhorar produtos externos para os clientes finais':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.5 AI Generativa e LLMs para melhorar produtos internos para os colaboradores':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.6 IA Generativa e LLMs como principal frente do negócio':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.7 IA Generativa e LLMs não é prioridade':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.l.8 Não sei opinar sobre o uso de IA Generativa e LLMs na empresa':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.1 Não uso soluções de AI Generativa com foco em produtividade':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.2 Uso soluções gratuitas de AI Generativa com foco em produtividade':\n",
            "[ 1. nan  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.3 Uso e pago pelas soluções de AI Generativa com foco em produtividade':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.4 A empresa que trabalho paga pelas soluções de AI Generativa com foco em produtividade':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '4.m.5 Uso soluções do tipo Copilot':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '5.b_oportunidade_buscada':\n",
            "[nan 'Engenheiro de Machine Learning/ML Engineer'\n",
            " 'Cientista de Dados/Data Scientist'\n",
            " 'Engenheiro de Dados/Arquiteto de Dados/Data Engineer/Data Architect'\n",
            " 'Analista de Dados/Data Analyst' 'Analista de Negócios/Business Analyst'\n",
            " 'Data Product Manager' 'Analista de BI/BI Analyst' 'Analytics Engineer'\n",
            " 'Devops, MLops, Cientista de Dados']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '5.c_tempo_em_busca_de_oportunidade':\n",
            "[nan '7 meses - 1 ano' '0 - 6 meses' '1 ano - 2 anos' 'acima de 2 anos']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '5.d_experiencia_em_processos_seletivos':\n",
            "[nan 'Ainda não me candidatei a nenhuma vaga na área'\n",
            " 'Já me candidatei, mas nunca fui chamado para entrevistas'\n",
            " 'Já participei de 1 a 3 entrevistas, mas não fui contratado'\n",
            " 'Participei de entrevistas e fui contratada'\n",
            " 'Participei de mais de 3 entrevistas, mas não fui contratado'\n",
            " 'Gostaria de migrar internamente na empresa em que trabalho como SWE, mais de um nível de gestão já sabe dessa intenção.'\n",
            " 'Readequando LinkedIn e Currículo'\n",
            " 'Me candidatei mas nunca chamaram para entrevista.'\n",
            " 'Não tenho formação e não tenho experiencia direta na area']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.1_Desenvolvo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.2_Realizo construções de ETL's em ferramentas como Pentaho, Talend, Dataflow etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.3_Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.4_Atuo na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.5_Modelo soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.6_Desenvolvo/cuido da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.7_Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.8_Cuido da qualidade dos dados, metadados e dicionário de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.a.9_Nenhuma das opções listadas refletem meu dia a dia.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.1_Scripts Python':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.2_SQL & Stored Procedures':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.3_Apache Airflow':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.4_Apache NiFi':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.5_Luigi':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.6_AWS Glue':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.7_Talend':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.8_Pentaho':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.9_Alteryx':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.10_Stitch':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.11_Fivetran':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.12_Google Dataflow':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.13_Oracle Data Integrator':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.14_IBM DataStage':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.15_SAP BW ETL':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.16_SQL Server Integration Services (SSIS)':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.17_SAS Data Integration':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.18_Qlik Sense':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.19_Knime':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.20_Databricks':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.b.21_Não utilizo ferramentas de ETL':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.c_possui_data_lake':\n",
            "[nan True False]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.d_tecnologia_data_lake':\n",
            "[nan 'Amazon S3 + Redshift + Athena' 'Azure Datalake'\n",
            " 'Google FS + BigQuery' 'Databricks' 'Snowflake' 'Hadoop + Hive' 'Presto']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.e_possui_data_warehouse':\n",
            "[nan True False]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.f_tecnologia_data_warehouse':\n",
            "[nan 'AWS Redshift' 'Azure' 'Google BigQuery' 'Databricks' 'Snowflake'\n",
            " 'Postgres/MySQL' 'Oracle' 'Presto' 'Teradata' 'IBM']\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.1_Desenvolvendo pipelines de dados utilizando linguagens de programação como Python, Scala, Java etc.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.2_Realizando construções de ETL\\s em ferramentas como Pentaho, Talend, Dataflow etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.3_Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.4_Atuando na integração de diferentes fontes de dados através de plataformas proprietárias como Stitch Data, Fivetran etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.5_Modelando soluções de arquitetura de dados, criando componentes de ingestão de dados, transformação e recuperação da informação.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.6_Desenvolvendo/cuidando da manutenção de repositórios de dados baseados em streaming de eventos como Data Lakes e Data Lakehouses.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.7_Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.8_Cuidando da qualidade dos dados, metadados e dicionário de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '6.h.9_Nenhuma das opções listadas refletem meu dia a dia.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.1_Processo e analiso dados utilizando linguagens de programação como Python, R etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.2_Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.3_Crio consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.4_Utilizo API\\s para extrair dados e complementar minhas análises.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.5_Realizo experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.6_Desenvolvo/cuido da manutenção de ETL\\s utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.7_Atuo na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.8_Desenvolvo/cuido da manutenção de planilhas para atender as áreas de negócio.':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.9_Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.a.10_Nenhuma das opções listadas refletem meu dia a dia.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.1_Scripts Python':\n",
            "[ 1. nan  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.2_SQL & Stored Procedures':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.3_Apache Airflow':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.4_Apache NiFi':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.5_Luigi':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.6_AWS Glue':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.7_Talend':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.8_Pentaho':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.9_Alteryx':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.10_Stitch':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.11_Fivetran':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.12_Google Dataflow':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.13_Oracle Data Integrator':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.14_IBM DataStage':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.15_SAP BW ETL':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.16_SQL Server Integration Services (SSIS)':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.17_SAS Data Integration':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.18_Qlik Sense':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.19_Knime':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.20_Databricks':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.b.21_Não utilizo ferramentas de ETL':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.1_Ferramentas de AutoML como H2O.ai, Data Robot, BigML etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.2_\"Point and Click\" Analytics como Alteryx, Knime, Rapidminer etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.3_Product metricts & Insights como Mixpanel, Amplitude, Adobe Analytics.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.4_Ferramentas de análise dentro de ferramentas de CRM como Salesforce Einstein Anaytics ou Zendesk dashboards.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.5_Minha empresa não utiliza essas ferramentas.':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.c.6_Não sei informar.':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.1_Processando e analisando dados utilizando linguagens de programação como Python, R etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.2_Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik etc.':\n",
            "[ 0.  1. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.3_Criando consultas através da linguagem SQL para exportar informações e compartilhar com as áreas de negócio.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.4_Utilizando API's para extrair dados e complementar minhas análises.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.5_Realizando experimentos e estudos utilizando metodologias estatísticas como teste de hipótese, modelos de regressão etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.6_Desenvolvendo/cuidando da manutenção de ETL's utilizando tecnologias como Talend, Pentaho, Airflow, Dataflow etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.7_Atuando na modelagem dos dados, com o objetivo de criar conjuntos de dados como Data Warehouses, Data Marts, Datasets etc.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.8_Desenvolvendo/cuidando da manutenção de planilhas para atender as áreas de negócio.':\n",
            "[ 1.  0. nan]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.9_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises de dados.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '7.d.10_Nenhuma das opções listadas refletem meu dia a dia.':\n",
            "[ 0. nan  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.1_Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.2_Sou responsável pela coleta e limpeza dos dados que uso para análise e modelagem.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.3_Sou responsável por entrar em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.4_Desenvolvo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.5_Sou responsável por colocar modelos em produção, criar os pipelines de dados, APIs de consumo e monitoramento.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.6_Cuido da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.7_Realizo construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.8_Utilizo ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.9_Crio e dou manutenção em ETLs, DAGs e automações de pipelines de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.10_Crio e gerencio soluções de Feature Store e cultura de MLOps.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.11_Sou responsável por criar e manter a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.a.12_Treino e aplico LLM's para solucionar problemas de negócio.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.1_Utilizo modelos de regressão (linear, logística, GLM).':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.2_Utilizo redes neurais ou modelos baseados em árvore para criar modelos de classificação.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.3_Desenvolvo sistemas de recomendação (RecSys).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.4_Utilizo métodos estatísticos Bayesianos para analisar dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.5_Utilizo técnicas de NLP (Natural Language Processing) para análisar dados não-estruturados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.6_Utilizo métodos estatísticos clássicos (Testes de hipótese, análise multivariada, sobrevivência, dados longitudinais, inferência estatistica) para analisar dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.7_Utilizo cadeias de Markov ou HMM\\s para realizar análises de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.8_Desenvolvo técnicas de Clusterização (K-means, Spectral, DBScan etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.9_Realizo previsões através de modelos de Séries Temporais (Time Series).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.10_Utilizo modelos de Reinforcement Learning (aprendizado por reforço).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.11_Utilizo modelos de Machine Learning para detecção de fraude.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.12_Utilizo métodos de Visão Computacional.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.13_Utilizo modelos de Detecção de Churn.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.b.14_Utilizo LLM's para solucionar problemas de negócio.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.1_Ferramentas de BI (PowerBI, Looker, Tableau, Qlik etc).':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.2_Planilhas (Excel, Google Sheets etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.3_Ambientes de desenvolvimento local (R-studio, JupyterLab, Anaconda).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.4_Ambientes de desenvolvimento na nuvem (Google Colab, AWS Sagemaker, Kaggle Notebooks etc).':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.5_Ferramentas de AutoML (Datarobot, H2O, Auto-Keras etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.6_Ferramentas de ETL (Apache Airflow, NiFi, Stitch, Fivetran, Pentaho etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.7_Plataformas de Machine Learning (TensorFlow, Azure Machine Learning, Kubeflow etc).':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.8_Feature Store (Feast, Hopsworks, AWS Feature Store, Databricks Feature Store etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.9_Sistemas de controle de versão (Github, DVC, Neptune, Gitlab etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.10_Plataformas de Data Apps (Streamlit, Shiny, Plotly Dash etc).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.c.11_Ferramentas de estatística avançada como SPSS, SAS, etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.1_Estudos Ad-hoc com o objetivo de confirmar hipóteses, realizar modelos preditivos, forecasts, análise de cluster para resolver problemas pontuais e responder perguntas das áreas de negócio.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.2_Coletando e limpando dos dados que uso para análise e modelagem.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.3_Entrando em contato com os times de negócio para definição do problema, identificar a solução e apresentação de resultados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.4_Desenvolvendo modelos de Machine Learning com o objetivo de colocar em produção em sistemas (produtos de dados).':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.5_Colocando modelos em produção, criando os pipelines de dados, APIs de consumo e monitoramento.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.6_Cuidando da manutenção de modelos de Machine Learning já em produção, atuando no monitoramento, ajustes e refatoração quando necessário.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.7_Realizando construções de dashboards em ferramentas de BI como PowerBI, Tableau, Looker, Qlik, etc.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.8_Utilizando ferramentas avançadas de estatística como SAS, SPSS, Stata etc, para realizar análises.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.9_Criando e dando manutenção em ETLs, DAGs e automações de pipelines de dados.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.10_Criando e gerenciando soluções de Feature Store e cultura de MLOps.':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.11_Criando e mantendo a infra que meus modelos e soluções rodam (clusters, servidores, API, containers, etc.)':\n",
            "[nan  0.  1.]\n",
            "----------------------------------------\n",
            "Valores únicos na coluna '8.d.12_Treinando e aplicando LLM's para solucionar problemas de negócio.':\n",
            "[nan  1.  0.]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Limite para considerar colunas com muitos valores ausentes (ex: acima de 70%)\n",
        "limite_percentual = 70\n",
        "\n",
        "# Calcular percentual de valores ausentes por coluna\n",
        "na_percent = df.isnull().mean() * 100\n",
        "\n",
        "# Filtrar colunas acima do limite definido\n",
        "colunas_com_muitos_nas = na_percent[na_percent > limite_percentual].sort_values(ascending=False)\n",
        "\n",
        "# Criar tabela com quantidade de registros válidos (não nulos)\n",
        "tabela_nas = pd.DataFrame({\n",
        "    \"Coluna\": colunas_com_muitos_nas.index,\n",
        "    \"% de NAs\": colunas_com_muitos_nas.values,\n",
        "    \"Qtd. válidos\": df.shape[0] - df[colunas_com_muitos_nas.index].isnull().sum().values\n",
        "})\n",
        "\n",
        "# Exibir resultado\n",
        "tabela_nas.reset_index(drop=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7IRvkYd8W5ND",
        "outputId": "3e09e3c6-bbf7-457e-fa5c-73258e85ffbc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       Coluna  % de NAs  Qtd. válidos\n",
              "0                          1.h_pais_onde_mora     97.34           139\n",
              "1                    5.b_oportunidade_buscada     94.52           286\n",
              "2      5.d_experiencia_em_processos_seletivos     94.46           289\n",
              "3          5.c_tempo_em_busca_de_oportunidade     94.44           290\n",
              "4               5.a_objetivo_na_area_de_dados     89.63           541\n",
              "..                                        ...       ...           ...\n",
              "185                   2.l_motivo_insatisfacao     70.73          1527\n",
              "186    2.l.4_Flexibilidade de trabalho remoto     70.73          1527\n",
              "187                 2.l.1_Remuneração/Salário     70.73          1527\n",
              "188                          2.l.2_Benefícios     70.73          1527\n",
              "189  2.l.3_Propósito do trabalho e da empresa     70.73          1527\n",
              "\n",
              "[190 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-273f877d-6534-4fea-891c-22646f8138fb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Coluna</th>\n",
              "      <th>% de NAs</th>\n",
              "      <th>Qtd. válidos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.h_pais_onde_mora</td>\n",
              "      <td>97.34</td>\n",
              "      <td>139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.b_oportunidade_buscada</td>\n",
              "      <td>94.52</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.d_experiencia_em_processos_seletivos</td>\n",
              "      <td>94.46</td>\n",
              "      <td>289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.c_tempo_em_busca_de_oportunidade</td>\n",
              "      <td>94.44</td>\n",
              "      <td>290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.a_objetivo_na_area_de_dados</td>\n",
              "      <td>89.63</td>\n",
              "      <td>541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>2.l_motivo_insatisfacao</td>\n",
              "      <td>70.73</td>\n",
              "      <td>1527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>2.l.4_Flexibilidade de trabalho remoto</td>\n",
              "      <td>70.73</td>\n",
              "      <td>1527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>2.l.1_Remuneração/Salário</td>\n",
              "      <td>70.73</td>\n",
              "      <td>1527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>2.l.2_Benefícios</td>\n",
              "      <td>70.73</td>\n",
              "      <td>1527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>2.l.3_Propósito do trabalho e da empresa</td>\n",
              "      <td>70.73</td>\n",
              "      <td>1527</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>190 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-273f877d-6534-4fea-891c-22646f8138fb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-273f877d-6534-4fea-891c-22646f8138fb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-273f877d-6534-4fea-891c-22646f8138fb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3177b9ad-c7cf-4c97-ad75-9718928c3087\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3177b9ad-c7cf-4c97-ad75-9718928c3087')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3177b9ad-c7cf-4c97-ad75-9718928c3087 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"tabela_nas\",\n  \"rows\": 190,\n  \"fields\": [\n    {\n      \"column\": \"Coluna\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 190,\n        \"samples\": [\n          \"1.f.8_Rela\\u00e7\\u00e3o com outras pessoas da empresa, em momentos de trabalho\",\n          \"2.l.5_Ambiente e clima de trabalho\",\n          \"3.g.2 Falta de confiabilidade das sa\\u00eddas (alucina\\u00e7\\u00e3o dos modelos)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"% de NAs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.310534419057493,\n        \"min\": 70.73030477285796,\n        \"max\": 97.33563350584627,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          97.33563350584627,\n          79.96933103316081,\n          80.83189572551275\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Qtd. v\\u00e1lidos\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 224,\n        \"min\": 139,\n        \"max\": 1527,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          139,\n          1045,\n          1000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Análise da distribuição por gênero\n",
        "genero_counts = df['1.b_genero'].value_counts(dropna=False)\n",
        "\n",
        "# Exibindo os resultados com formatação\n",
        "print(\"Distribuição por Gênero:\")\n",
        "print(genero_counts)\n",
        "print(\"\\nPercentual por Gênero:\")\n",
        "print((genero_counts / genero_counts.sum() * 100).round(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt0EnSdFMrzy",
        "outputId": "c6145298-5e99-4c42-e25b-cc060737ad31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuição por Gênero:\n",
            "1.b_genero\n",
            "Masculino               3968\n",
            "Feminino                1226\n",
            "Prefiro não informar      15\n",
            "Outro                      8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentual por Gênero:\n",
            "1.b_genero\n",
            "Masculino              76.06\n",
            "Feminino               23.50\n",
            "Prefiro não informar    0.29\n",
            "Outro                   0.15\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['1.b_genero'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "c1RkG6FUtnFx",
        "outputId": "3ba4aba4-8265-4ab0-b8a4-142fdcc81668"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='1.b_genero'>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribuição por gênero\n",
        "genero_counts = df['1.b_genero'].value_counts(dropna=False)\n",
        "genero_percent = (genero_counts / genero_counts.sum()) * 100\n",
        "\n",
        "\n",
        "# Gráfico de pizza\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = sns.color_palette(\"Set2\", len(genero_counts))\n",
        "plt.pie(genero_percent, labels=None, autopct='%1.1f%%', startangle=140, colors=colors, pctdistance=0.5) # Adjusted pctdistance\n",
        "plt.title(\"Distribuição Percentual por Gênero\")\n",
        "plt.axis('equal')  # Mantém o formato de círculo\n",
        "plt.legend(labels=[f'{label} ({percent:.1f}%)' for label, percent in zip(genero_percent.index, genero_percent)], loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "QHe22U3GWIos",
        "outputId": "4f4170a2-df9f-4dda-9591-32b8ce5cea16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['1.a_idade'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "ULYrYKNDrAzV",
        "outputId": "bba7450e-6bee-449b-b925-8feefd09a8df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count   5,217.00\n",
              "mean       32.36\n",
              "std         7.42\n",
              "min        18.00\n",
              "25%        27.00\n",
              "50%        31.00\n",
              "75%        36.00\n",
              "max        68.00\n",
              "Name: 1.a_idade, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1.a_idade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5,217.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>32.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>18.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>27.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>31.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>36.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>68.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotando o Boxplot da variável Idade\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=df['1.a_idade'])\n",
        "plt.title('Boxplot da Variável Idade')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "o6lcqasyGN8z",
        "outputId": "dfac7e72-2be4-4e08-8a55-63a68c29b498"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Análise da distribuição por Raça / Etnia\n",
        "etnic_counts = df['1.c_cor/raca/etnia'].value_counts(dropna=False)\n",
        "\n",
        "# Exibindo os resultados com formatação\n",
        "print(\"Distribuição por cor/raca/etnia:\")\n",
        "print(etnic_counts)\n",
        "print(\"\\nPercentual por cor/raca/etnia:\")\n",
        "print((etnic_counts / etnic_counts.sum() * 100).round(2))"
      ],
      "metadata": {
        "id": "akIGyKKaHkXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f06337-0322-4c42-a517-b6f80537a271"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuição por cor/raca/etnia:\n",
            "1.c_cor/raca/etnia\n",
            "Branca                  3478\n",
            "Parda                   1180\n",
            "Preta                    352\n",
            "Amarela                  155\n",
            "Prefiro não informar      37\n",
            "Indígena                  10\n",
            "Outra                      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentual por cor/raca/etnia:\n",
            "1.c_cor/raca/etnia\n",
            "Branca                 66.67\n",
            "Parda                  22.62\n",
            "Preta                   6.75\n",
            "Amarela                 2.97\n",
            "Prefiro não informar    0.71\n",
            "Indígena                0.19\n",
            "Outra                   0.10\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbe7b978",
        "outputId": "851f2ff5-68d4-4362-e37d-bb461dcb3216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Análise da distribuição por Gênero, Raça / Etnia e UF onde mora\n",
        "\n",
        "# Distribuição por Gênero\n",
        "genero_counts = df['1.b_genero'].value_counts(dropna=False)\n",
        "print(\"Distribuição por Gênero:\")\n",
        "print(genero_counts)\n",
        "print(\"\\nPercentual por Gênero:\")\n",
        "print((genero_counts / genero_counts.sum() * 100).round(2))\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Distribuição por Raça / Etnia\n",
        "etnic_counts = df['1.c_cor/raca/etnia'].value_counts(dropna=False)\n",
        "print(\"Distribuição por cor/raca/etnia:\")\n",
        "print(etnic_counts)\n",
        "print(\"\\nPercentual por cor/raca/etnia:\")\n",
        "print((etnic_counts / etnic_counts.sum() * 100).round(2))\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Distribuição por UF onde mora\n",
        "uf_counts = df['1.i.1_uf_onde_mora'].value_counts(dropna=False)\n",
        "print(\"Distribuição por UF onde mora:\")\n",
        "print(uf_counts)\n",
        "print(\"\\nPercentual por UF onde mora:\")\n",
        "print((uf_counts / uf_counts.sum() * 100).round(2))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuição por Gênero:\n",
            "1.b_genero\n",
            "Masculino               3968\n",
            "Feminino                1226\n",
            "Prefiro não informar      15\n",
            "Outro                      8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentual por Gênero:\n",
            "1.b_genero\n",
            "Masculino              76.06\n",
            "Feminino               23.50\n",
            "Prefiro não informar    0.29\n",
            "Outro                   0.15\n",
            "Name: count, dtype: float64\n",
            "----------------------------------------\n",
            "Distribuição por cor/raca/etnia:\n",
            "1.c_cor/raca/etnia\n",
            "Branca                  3478\n",
            "Parda                   1180\n",
            "Preta                    352\n",
            "Amarela                  155\n",
            "Prefiro não informar      37\n",
            "Indígena                  10\n",
            "Outra                      5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentual por cor/raca/etnia:\n",
            "1.c_cor/raca/etnia\n",
            "Branca                 66.67\n",
            "Parda                  22.62\n",
            "Preta                   6.75\n",
            "Amarela                 2.97\n",
            "Prefiro não informar    0.71\n",
            "Indígena                0.19\n",
            "Outra                   0.10\n",
            "Name: count, dtype: float64\n",
            "----------------------------------------\n",
            "Distribuição por UF onde mora:\n",
            "1.i.1_uf_onde_mora\n",
            "SP     2064\n",
            "MG      580\n",
            "PR      415\n",
            "RJ      397\n",
            "RS      369\n",
            "SC      247\n",
            "DF      186\n",
            "NaN     142\n",
            "CE      128\n",
            "PE      120\n",
            "BA       99\n",
            "ES       91\n",
            "GO       72\n",
            "PB       53\n",
            "MT       44\n",
            "MS       30\n",
            "RN       30\n",
            "MA       28\n",
            "AL       23\n",
            "AM       23\n",
            "SE       21\n",
            "PA       18\n",
            "PI       14\n",
            "RO       13\n",
            "TO        6\n",
            "AP        4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentual por UF onde mora:\n",
            "1.i.1_uf_onde_mora\n",
            "SP    39.56\n",
            "MG    11.12\n",
            "PR     7.95\n",
            "RJ     7.61\n",
            "RS     7.07\n",
            "SC     4.73\n",
            "DF     3.57\n",
            "NaN    2.72\n",
            "CE     2.45\n",
            "PE     2.30\n",
            "BA     1.90\n",
            "ES     1.74\n",
            "GO     1.38\n",
            "PB     1.02\n",
            "MT     0.84\n",
            "MS     0.58\n",
            "RN     0.58\n",
            "MA     0.54\n",
            "AL     0.44\n",
            "AM     0.44\n",
            "SE     0.40\n",
            "PA     0.35\n",
            "PI     0.27\n",
            "RO     0.25\n",
            "TO     0.12\n",
            "AP     0.08\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análises e filtros para realizar no data frame:\n",
        "\n",
        "- criar um data frame contendo todas as colunas de identificação do individuo\n",
        "\n",
        "(0.a_token[0],\n",
        " 0.d_data/hora_envio[1],\n",
        " 1.a_idade[2],\n",
        " 1.a.1_faixa_idade[3],\n",
        " 1.b_genero[4],\n",
        " 1.c_cor/raca/etnia[5],\n",
        " 1.d_pcd[6],\n",
        " 1.i.1_uf_onde_mora[12],\n",
        " 1.i.2_regiao_onde_mora[22],\n",
        " 1.k.1_uf_de_origem[24],\n",
        " 1.k.2_regiao_de_origem[25],\n",
        " 1.l_nivel_de_ensino[31],\n",
        " 1.m_Área_de_formação[32],\n",
        " 2.a_situação_de_trabalho[33],\n",
        " 2.h_faixa_salarial[40],\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ideias:\n",
        "- Analisar a distribuição da faixa salarial por gênero;\n",
        "- Analisar A relação do tempo de experiência em dados e a faixa salarial;\n",
        "- Analisar a quantidade de empresas com times de dados e a quantidade de pessoas nesses times (colunas 74, 73, ;"
      ],
      "metadata": {
        "id": "oXsHLVg4AgMo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XWohrDGAiEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}